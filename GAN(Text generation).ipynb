{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness is required while using text generation so that the answers generated by the model are less predictable and more creative\n",
    "# However, there should be a balance in the randomness ( low randomness means predictable boring answers, high randomness means hypothetical creative answers which don't make any sense, so we need a intermediate randomness)\n",
    "# Softmax is used to predict the next word using the probability of the words ( By introducing randomness, if a word has 0.4 , then the word will be chosen 40 percent of the time)\n",
    "# To control the randomness , we use a term called \"softmax temperature\" - 0.0 to 1.0 range --> 0 means low entropy i.e boring answers or no randomness, 1.0 means high entropy i.e hypothetical creative answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the softmax temperature function works\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution,softmax_temperature=0.5): # Original distribution is 1D numpy array where probabilities sum up to 1 ( probability of words to be the next word in the sentence)\n",
    "\n",
    "    new_distribution = np.log(original_distribution) / softmax_temperature\n",
    "    new_distribution = np.exp(new_distribution)\n",
    "    return new_distribution / np.sum(new_distribution) # Returns a reweighted version of the original distribution( The sum might not be equal to 1, so to make it equal to 1 , we divide by the sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100006 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Using IMDB dataset\n",
    "\n",
    "# Downloading the dataset\n",
    "#dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\",\n",
    "#                                  origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "#                                  extract=True)\n",
    "IMDB_DATASET = r'C:\\Users\\arany\\.keras\\datasets\\aclimdb'\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(directory=IMDB_DATASET,label_mode=None,batch_size=256) # Since we need a huge amount of data and we dont want any classification tasks, so we dont need the labels and classes(label_mode = None)\n",
    "\n",
    "dataset = dataset.map(lambda x : tf.strings.regex_replace(x,\"<br />\",\" \")) # Removing the <br> html tags present in the reviews(Since we are just generating words, so not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorization ( creating the vocabulary for text generation)\n",
    "\n",
    "sequence_length = 100\n",
    "vocab_size = 15000 # Top 15000 frequent words are used, else treated as [UNK]\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset) # Using only the text reviews we extracted earlier to adapt our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a layer where the input sequence will the input itself as a tensor, but the target will be offset by 1(since we are training our model for generating the next word)\n",
    "\n",
    "def prepare_lm_dataset(text_batch):\n",
    "    vectorized_sequence = text_vectorization(text_batch) # Converting words to vector\n",
    "    source_sequence = vectorized_sequence[:,:-1] # Creating inputs by cutting off the last word of the sequence\n",
    "    target_sequence = vectorized_sequence[:,1:] # Creating targets by offsetting the sequences by 1\n",
    "    return source_sequence, target_sequence\n",
    "\n",
    "lm_dataset = dataset.map(prepare_lm_dataset,num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we do offsetting and cutting off in inputs and target\n",
    "\n",
    "### Suppose the below matrix representes the vectorization of the words in the sequence\n",
    "\n",
    "vectorized_sequence = [\n",
    "    [0.1, 0.2, 0.3, 0.4],  # The\n",
    "    [0.5, 0.6, 0.7, 0.8],  # cat\n",
    "    [0.9, 1.0, 1.1, 1.2],  # sat\n",
    "    [1.3, 1.4, 1.5, 1.6],  # on\n",
    "    [1.7, 1.8, 1.9, 2.0],  # the\n",
    "    [2.1, 2.2, 2.3, 2.4],  # mat\n",
    "    [0.1, 0.2, 0.3, 0.4],  # The\n",
    "    [2.5, 2.6, 2.7, 2.8],  # dog\n",
    "    [2.9, 3.0, 3.1, 3.2],  # barked\n",
    "    [3.3, 3.4, 3.5, 3.6]   # loudly\n",
    "]\n",
    "\n",
    "source_sequence = [\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6],\n",
    "    [1.7, 1.8, 1.9, 2.0],\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [2.5, 2.6, 2.7, 2.8],\n",
    "    [2.9, 3.0, 3.1, 3.2]\n",
    "]\n",
    "\n",
    "target_sequence = [\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6],\n",
    "    [1.7, 1.8, 1.9, 2.0],\n",
    "    [2.1, 2.2, 2.3, 2.4],\n",
    "    [2.5, 2.6, 2.7, 2.8],\n",
    "    [2.9, 3.0, 3.1, 3.2],\n",
    "    [3.3, 3.4, 3.5, 3.6]\n",
    "]\n",
    "\n",
    "### So, the line 1 of source_sequence is passed to model as input, the 1st line of target_sequence will be passed as output i.e the next word in the sentence\n",
    "### So each line number 'X' or Xth word being passed from source sequence to model has line number 'X' in target sequence which in turn is the X + 1th word in the sentence\n",
    "### So at each input and target pair, the previous word is the input and the next word is the output\n",
    "### Thats why we offset the target sequence by 1\n",
    "### We cut of the last word in input sequence because there is no next word to be learned by the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use sequence to sequence modelling with the help of encoder and decoder as used previously,, but here in case of text generation, we wont have any source sequence.\n",
    "# We are just trying to predict the next tokens using the past tokens with help of decoder\n",
    "# Causal padding helps in decoder only looking in the 0 to Nth token/ words to predict the N + 1 th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no source sequence , we will only be using the decoder part ( decoder only model)\n",
    "# Also we will be using positional embedding from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim,**kwargs): # The sequence length needs to be known because we need to use that as input dimension for the Positional embedding\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.token_embeddings = Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "        self.positional_embeddings = Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1] # Retreiving the length of the sequence\n",
    "        positions = tf.range(start=0,limit=length,delta=1) # List of number positions (1,2,3,4.....length of the sentence)\n",
    "        embedded_tokens = self.token_embeddings(inputs) # Word embeddings\n",
    "        embedded_positions = self.positional_embeddings(positions) # Position embeddings\n",
    "        return embedded_tokens + embedded_positions # Adding word and position embeddings\n",
    "\n",
    "    def compute_mask(self,inputs,mask=None): # Creating a mask to be able to ignore the zero paddings\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "\n",
    "    def get_config(self): # Created so that we can use this custom class later as a layer\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return config   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderClass(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.attention_layer1 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.attention_layer2 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_block = Sequential([\n",
    "            Dense(units=dense_dim,activation=\"relu\"),\n",
    "            Dense(units=embed_dim)\n",
    "        ])\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "        self.layer_norm3 = LayerNormalization()\n",
    "        self.supports_masking = True # Ensures that layer will propogate its input mask to its output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    # Causal padding ensures that during self-attention calculations in the transformer, each token only attends to the previous tokens in the sequence, not the future ones.\n",
    "    def get_causal_attention_mask(self,inputs): # Causual padding implementation ( Since the transformer model has access to the whole sequence , so that it doesn't directly copy while predicting the N+1 token, we pad the future elements in the sequence)\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size , sequence_length = input_shape[0] , input_shape[1]\n",
    "        # These lines generate two tensors i and j, where i represents a range from 0 to sequence_length - 1 along columns and j represents the same range along rows.\n",
    "        i = tf.range(sequence_length)[:,tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j,dtype=\"int32\") # This line creates a mask where each element is 1 if the corresponding element in i is greater than or equal to the corresponding element in j, and 0 otherwise. \n",
    "        # This ensures that each token only attends to itself and the previous tokens, not the future ones.\n",
    "        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1])) # Reshaping done so that the mask has the correct dimensions to be compatible with the subsequent tiling operation and matches the shape expected by the attention mechanism in the transformer model.\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size,-1), # This line helps us determine how many times the mask matrix will be repeated during tiling\n",
    "             tf.constant([1,1],dtype=\"int32\")],axis=0) # Here [1,1] means that the mask matrix will not be tilled in other dimensions\n",
    "        return tf.tile(mask,mult) # Tiling process means copying the mask matrix for different dimensions(here only 1 dimension whose number of times to be replicated depends on the batch size)\n",
    "    \n",
    "    def call(self,inputs,encoder_outputs,mask=None): # inputs is the target sequence provided to decoder as input, encoder_inputs is the representation of the source sequence of the encoder\n",
    "        causal_mask = self.get_causal_attention_mask(inputs) # Retreiving the causal mask\n",
    "        \n",
    "        # If a padding mask is provided, it's first cast to an integer type and expanded to match the shape of the causal mask. \n",
    "        #Then, a minimum operation is performed element-wise between the padding mask and the causal mask. \n",
    "        #This step ensures that the model doesn't attend to the padded elements during the attention calculation.\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,tf.newaxis,:],dtype=\"int32\") # Preparing the input mask which describes the padding locations in the target sequence\n",
    "            padding_mask = tf.minimum(padding_mask,causal_mask) # Merging the masks together\n",
    "        \n",
    "        # Attention layer 1 has only the inputs sent to the decoder, so the inputs will be the query, key and value for the layer\n",
    "        # Causal mask only applied here because the model only has the source sequence \n",
    "        attention_output_1 = self.attention_layer1(query=inputs,key=inputs,value=inputs,attention_mask=causal_mask) # Pass the causal mask to the first attention layer, which performs self attention over target sequence        \n",
    "        attention_output_1 = self.layer_norm1(inputs + attention_output_1) # Applying layer normalization and residual connection\n",
    "        \n",
    "        # Attention layer 2 has the attention scores and outputs from the previous attention layer which will be the query here, and the outputs sent by the encoder will be the value and key here ( since we are using context information from the encoder as the key and corresponding values to predict the next token)\n",
    "        # Padding mask is used since the model has both target and source sequence here\n",
    "        attention_output_2 = self.attention_layer2(query=attention_output_1,key=encoder_outputs,value=encoder_outputs,attention_mask=padding_mask) # Pass the padding mask to the second attention layer, which relates the source sequence to the target sequence\n",
    "        attention_output_2 = self.layer_norm2(attention_output_1 + attention_output_2) # Applying layer normalization and residual connection\n",
    "        \n",
    "        proj_output = self.dense_block(attention_output_2) # Dense layer block\n",
    "        return self.layer_norm3(attention_output_2 + proj_output) # Apply layer normalization and residual connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,Embedding,MultiHeadAttention,LayerNormalization\n",
    "from keras.models import Model\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 1024\n",
    "num_heads = 2\n",
    "\n",
    "input_layer = Input(shape=(None,),dtype=\"int64\")\n",
    "positional_embedding_layer = PositionalEmbedding(sequence_length=sequence_length,input_dim=vocab_size,output_dim=embed_dim)(input_layer)\n",
    "decoder_layer = TransformerDecoderClass(embed_dim=embed_dim,dense_dim=dense_dim,num_heads=num_heads)(positional_embedding_layer,positional_embedding_layer) # Passing the positional embedding layer twice in the decoder layer is necessary for allowing the self-attention mechanism to consider both the input tokens and their positional information when generating the output sequence in your text generation model\n",
    "# Since we don't have a source sequence and an encoder, so we need to get both the positional information and the previous tokens both from positional embedding\n",
    "output_layer = Dense(vocab_size,activation=\"softmax\")(decoder_layer) # Vocab_size and softmax activation so that we get a probability distribution of each word present in the vocabulary that it can be the next token\n",
    "\n",
    "decoder_model = Model(input_layer,output_layer)\n",
    "decoder_model.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(self): # Function created to return the dictionary ( Facing 'utf-8' decode issue while using get_vocabulary method for text vectorization)\n",
    "        # _layer.get_vocabulary\n",
    "        keys, values = self._lookup_layer.lookup_table.export()\n",
    "        # print(self._lookup_layer.lookup_table.export())\n",
    "        vocab = []\n",
    "        for i in keys : \n",
    "            try :\n",
    "                vocab.append(i.numpy().decode('utf-8'))\n",
    "            except :\n",
    "                vocab.append(i.numpy().decode('ISO-8859-1'))\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_index = dict(enumerate(get_vocabulary(text_vectorization))) # We are creating a dictionary that maps the word indices to strings(used for text decoding)\n",
    "def sample_next(predictions, temperature=1.0): # Function for implementing variable temperature sampling from a probability distribution ( the first code block in the notebook)\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "class TextGenerator(keras.callbacks.Callback): # We will use this as a callback to fit our model\n",
    "    def __init__(self,\n",
    "        prompt, # prompt that will be model input for text generation\n",
    "        generate_length, # number of words to generate\n",
    "        model_input_length, # length of the inputs we used to train the model\n",
    "        temperatures=(1.,), # range of temperatures\n",
    "        print_freq=1,\n",
    "        model=decoder_model): # Our decoder model mentioned here\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        for temperature in self.temperatures:\n",
    "            print(\"== Generating with temperature\", temperature)\n",
    "            sentence = self.prompt # our prompt(input for text generation) will be the initial part of the final output sequence \n",
    "            for i in range(self.generate_length):\n",
    "                tokenized_sentence = text_vectorization([sentence])\n",
    "                predictions = self.model(tokenized_sentence) # next word prediction probability distribution by the model\n",
    "                next_token = sample_next(predictions[0, i, :]) # using the prediction of the model with the temperature range to get the next token\n",
    "                sampled_token = tokens_index[next_token]\n",
    "                sentence += \" \" + sampled_token # Adding the next word in the sentence ( which acts as the input prompt for the next epoch)\n",
    "            print(sentence)\n",
    "\n",
    "prompt = \"This movie\" # Initial prompt\n",
    "text_gen_callback = TextGenerator( #Callback function will call this Text Generator class\n",
    "prompt,\n",
    "generate_length=50,\n",
    "model_input_length=sequence_length,\n",
    "temperatures=(0.2, 0.5, 0.7, 1., 1.5)) # Range of temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  6/391 [..............................] - ETA: 2:26 - loss: 5.0271WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1703s vs `on_train_batch_end` time: 0.2032s). Check your callbacks.\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.8804== Generating with temperature 0.2\n",
      "This movie defy criteria glamorous consequences preaching construct nightmare iron thornton illegal gigantic awe warrant consequences locke annoys proved fantastically captive jacqueline door scheming portraits warrant nightmare locke maiden warrant paintings locke locke confronts demand blonde thwarted warrant suffered glamorous locke meg suffered certain futuristic ole corridors somethings skin witherspoon warrant groan\n",
      "== Generating with temperature 0.5\n",
      "This movie loud dutch shapes stalking notices duplicate beggars 16mm evie confined unreal consequences of pretensions wade eager updated consequences skin 16mm fuss siege ran suffered drawback misfire former locke misfortune criteria finland thwarted spencer suffered destined bandits consequences handsome skin skin former politician climate bettany biko wakes addresses sorority david agents\n",
      "== Generating with temperature 0.7\n",
      "This movie bimbos criteria india criteria forwarded thwarted wholly bye peer magnetic reflections maiden biko gosh door warrant warrant skin 16mm cobb officer illfated skin maker americans score compete den novice warrant warrant shrek guys amos thornton warrant suffered imprisoned convinced thwarted beggars suffered your 16mm engaging worldly funded lingering spilled abigail\n",
      "== Generating with temperature 1.0\n",
      "This movie criteria betty futuristic criteria angus criteria develops biko 16mm imitations skin nightmare criteria officer officer ateam bye 16mm karen boob warrant tricked 16mm skies 16mm invent vcr comparable skin consequences lethargic thwarted permission suffered consequences 16mm captivating warrant proved kitty beggars skin evie completely rap cynthia warrant frighteningly 16mm grandson\n",
      "== Generating with temperature 1.5\n",
      "This movie criteria cursing thwarted recover cunning 16mm performs random 16mm kudos defy consequences angles gong locke cobb blockbusters 16mm consequences bye pictures warrant jen misfire warrant wakes warrant sorority locke skin criteria kristin imitations ever nightmare inflict masterfully grab prepare novice consequences those transforming moviethe sequences warrant bye visit 16mm delves\n",
      "391/391 [==============================] - 765s 2s/step - loss: 4.8804\n",
      "Epoch 2/2\n",
      "391/391 [==============================] - ETA: 0s - loss: 4.6333== Generating with temperature 0.2\n",
      "This movie defy criteria 16mm progressive warrant kudos component consequences consequences nightmare skin biko ignored hess vcr babysitter competitors locke tubes life door door echoes passenger beggars generals evie groan lawyer skin unpleasant italian tenderness subplots door 16mm wakes terrifically sorority skin media door bout wasnt bride backseat outside consequences nightmare thwarted\n",
      "== Generating with temperature 0.5\n",
      "This movie criteria develops consequences marvelously glove sought warrant consequences piles mountains warrant door 16mm consequences prostitution despicable also bed ridiculously beggars high evie hookers impress pictures custom insect 16mm warrant buttons misfire 16mm vcr armed grain beam nightmare joyce cohesive certain manner ripping criteria amazingly historian people pole warrant skin locke\n",
      "== Generating with temperature 0.7\n",
      "This movie reviewed locke maria omitted peasant circles woo storyline skin 16mm rugby confined door consequences sees thwarted imprisoned suffered 16mm wholly connected exceedingly suffered emphasized door construct disposal nightmare nightmare brighton grasp locke nightmare consequences biko effect eric occurred jewel consequences andor 16mm bats skin gooding miike criteria translated former consequences\n",
      "== Generating with temperature 1.0\n",
      "This movie criteria score criteria criteria futuristic thwarted redgrave door warrant intrusive jerks ho groan 16mm vcr oz hubby develops survivors bye hallway warrant caron locke pope criteria formal thwarted warrant suffered epilogue locke stilted skin suffered vomiting magnificently whimsical warrant handsome glamorous warrant sessions kotto skin door delight skin skin and\n",
      "== Generating with temperature 1.5\n",
      "This movie loud develops ambitious biko loner criteria infuriating nightmare vets joyce detract warrant futuristic evolution warrant futuristic ahead deserts taxes skin skin tolerant poster fried thornton warrant criteria plastic thwarted tomas suffered emergency thornton deanna door warrant roller vulnerable suggested jefferson 1973 breakthrough ill oz mountain 16mm overboard dwell consequences release\n",
      "391/391 [==============================] - 428s 1s/step - loss: 4.6333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df5a96d5d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_model.fit(lm_dataset,epochs=2,callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
