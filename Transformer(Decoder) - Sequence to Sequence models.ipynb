{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine translation use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"spa-eng.zip\",origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "DATASET_DIR = r\"C:\\Users\\arany\\.keras\\datasets\\spa-eng\"\n",
    "base_path = pathlib.Path(DATASET_DIR)\n",
    "\n",
    "text_filepath = base_path / \"spa.txt\"\n",
    "with open(text_filepath) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    english , spanish = line.split(\"\\t\") # Since each line contains the english and spanish sentence as tab seperated \n",
    "    spanish = \"[start] \" + spanish + \" [end]\" # So that we get a start and end sentence tokens for each spanish word (used in decoder)\n",
    "    text_pairs.append((english,spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"I don't know who wrote it.\", '[start] No sé quién lo escribió. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test and Validation split\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = int(len(text_pairs) - 2*num_val_samples)\n",
    "num_test_samples = num_train_samples - num_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples+ num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create 2 seperate text vectorization for 2 different language(punctuations may be different) ( Also the brackets might get removed from spanish translation in [start] and [end], but we need them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\" # Adding the character as punctuation\n",
    "strip_chars = strip_chars.replace(\"[\",\"\") # Removing brackets from string punctuations\n",
    "strip_chars = strip_chars.replace(\"]\",\"\")\n",
    "\n",
    "def custom_standardization(input_string): # Function for lower case conversioon and removing and adding punctuation for the spanish language\n",
    "    output_string = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(output_string,f\"[{re.escape(strip_chars)}]\",\"\") # Removing the punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000 # top 15000 frequent words\n",
    "sequence_length = 20 # Sequence length restricted to 20 words\n",
    "\n",
    "# Vectorization of the source sequence(english) and the target sequence(spanish)\n",
    "\n",
    "source_vectorization = TextVectorization(max_tokens=vocab_size,output_mode=\"int\",output_sequence_length=sequence_length)\n",
    "\n",
    "# Generating spanish sequences with 1 extra token per sentence because we need to offset the sentence by one step during training\n",
    "# If both source and target have the same number of tokens, then +1 added in the output sequence length means it will be predicting the next token..i,e the 4th token\n",
    "# If we don't add +1 , then since both the source and target have same length, then there is no new next token to predict( so we add + 1 to the output_sequence_length)\n",
    "target_vectorization = TextVectorization(max_tokens=vocab_size,output_mode=\"int\",output_sequence_length=sequence_length + 1,standardize=custom_standardization) \n",
    "\n",
    "# Extracting the texts seperately and training our vocabularies seperately for english and spanish\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data pipeline should return the below tuple\n",
    "# (inputs,target) where inputs = {\"encoder_inputs\":\"english sentence from the input file\" , \"decoder_inputs\" : \"spanish sentence from the input file\"}\n",
    "# target is the Spanish sentence offset by one step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset \n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng,spa): # Function to get the tuple\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\" : eng ,\n",
    "        \"spanish\" : spa[:,:-1] # The input spanish sequence doesn't include the last token to keep the inputs and targets of same length\n",
    "    }, spa[:,1:]) # The target spanish sequence is one step ahead(Both are still the same length)\n",
    "\n",
    "def make_dataset(pairs): \n",
    "    eng_texts,spa_texts = zip(*pairs) # Unzips the pairs into separate lists of English and Spanish sentences.\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts,spa_texts)) # Converts the lists into TensorFlow dataset\n",
    "    dataset = dataset.batch(batch_size) # Batches the dataset\n",
    "    dataset = dataset.map(format_dataset,num_parallel_calls=4) # This function formats each pair of sentences into the required format(format defined in format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache() # Prefetches 16 batches of data to speed up training\n",
    "\n",
    "train_dataset = make_dataset(train_pairs)\n",
    "\n",
    "val_dataset = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_dataset.take(1):\n",
    "    print(inputs[\"english\"].shape)\n",
    "    print(inputs[\"spanish\"].shape)\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model : Need two RNN components(encoder and decoder) --> encoder will turn the entire sequence into a single or set of vectors ---> This single or set of vectors will be used as initial state\n",
    "# for the decoder, which will look at elements 0 to N in target sequence and try to predict the N+1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GRU instead of LSTM because it makes things simpler,, since a single state vector is used in GRU as compared to multi set vector in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Dense, Input, Embedding, Bidirectional, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " spanish (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, None, 256)    3840000     ['english[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, None, 256)    3840000     ['spanish[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 1024)        7876608     ['embedding_4[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " gru_5 (GRU)                    (None, None, 1024)   3938304     ['embedding_5[0][0]',            \n",
      "                                                                  'bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 1024)   0           ['gru_5[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 15000)  15375000    ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,869,912\n",
      "Trainable params: 34,869,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "\n",
    "# Encoder\n",
    "source = Input(shape=(None,),dtype=\"int64\",name=\"english\") # English Source sentence\n",
    "embedding_layer1 = Embedding(input_dim=vocab_size,output_dim=embed_dim,mask_zero=True)(source) # Masking is a critical step needed( Sentences with variable lengths will be padded)\n",
    "encoded_source = Bidirectional(GRU(latent_dim),merge_mode=\"sum\")(embedding_layer1) # Output layer of our encoder(\"sum\" means the forward and backward direction representations will be summed together to get the final output encoded representations)\n",
    "\n",
    "# Decoder\n",
    "past_target = Input(shape=(None,),dtype=\"int64\",name=\"spanish\") # Spanish Target sentence\n",
    "embedding_layer2 = Embedding(input_dim=vocab_size,output_dim=embed_dim,mask_zero=True)(past_target) # Masking is critical here as well\n",
    "decoded_gru = GRU(latent_dim,return_sequences=True) # Specifying the units of the GRU layer and we need the full sequence of outputs generated by the decoder not just the final last output , so return_sequences is set to True\n",
    "\n",
    "# The below layer is the main decoder GRU layer and we are initializing it by passing the information that we got as output from our encoder\n",
    "# So the decoder can use the target sequence to predict new tokens using the context information that was learned by the encoder while it was learning on the source input\n",
    "decoded_gru_initialized = decoded_gru(embedding_layer2,initial_state=encoded_source) # Encoder source sequence set as the initial state for the decoder GRU  \n",
    "dropout_layer = Dropout(0.5)(decoded_gru_initialized)\n",
    "target_next_token_layer = Dense(vocab_size,activation=\"softmax\")(dropout_layer)\n",
    "seq2seq_rnn = Model([source,past_target],target_next_token_layer) # Inputs are source and past_target layers , output layer is target_next_token_layer\n",
    "\n",
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 652s 490ms/step - loss: 1.6282 - accuracy: 0.4195 - val_loss: 1.3128 - val_accuracy: 0.5070\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 603s 463ms/step - loss: 1.3111 - accuracy: 0.5281 - val_loss: 1.1497 - val_accuracy: 0.5686\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 96s 74ms/step - loss: 1.1695 - accuracy: 0.5776 - val_loss: 1.0706 - val_accuracy: 0.5998\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0809 - accuracy: 0.6092 - val_loss: 1.0392 - val_accuracy: 0.6190\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 94s 72ms/step - loss: 1.0323 - accuracy: 0.6339 - val_loss: 1.0231 - val_accuracy: 0.6280\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 94s 72ms/step - loss: 1.0015 - accuracy: 0.6520 - val_loss: 1.0205 - val_accuracy: 0.6353\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 96s 74ms/step - loss: 0.9830 - accuracy: 0.6657 - val_loss: 1.0197 - val_accuracy: 0.6392\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 98s 75ms/step - loss: 0.9691 - accuracy: 0.6761 - val_loss: 1.0247 - val_accuracy: 0.6406\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 98s 75ms/step - loss: 0.9594 - accuracy: 0.6835 - val_loss: 1.0246 - val_accuracy: 0.6421\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 100s 77ms/step - loss: 0.9521 - accuracy: 0.6899 - val_loss: 1.0283 - val_accuracy: 0.6420\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 101s 78ms/step - loss: 0.9466 - accuracy: 0.6937 - val_loss: 1.0310 - val_accuracy: 0.6427\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 103s 79ms/step - loss: 0.9427 - accuracy: 0.6967 - val_loss: 1.0324 - val_accuracy: 0.6437\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 103s 79ms/step - loss: 0.9400 - accuracy: 0.6993 - val_loss: 1.0348 - val_accuracy: 0.6440\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 103s 79ms/step - loss: 0.9379 - accuracy: 0.7007 - val_loss: 1.0362 - val_accuracy: 0.6432\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 105s 81ms/step - loss: 0.9382 - accuracy: 0.7009 - val_loss: 1.0398 - val_accuracy: 0.6432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x173f5c50490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=\"accuracy\")\n",
    "seq2seq_rnn.fit(train_dataset,epochs=15,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Good words are worth a lot, but cost almost nothing.\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] las [UNK] mucho que las mujeres [UNK] mucho pero no son las cosas más tarde [end]\n",
      "-------------\n",
      "We all like cycling.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] a todos nos gusta el extranjero [end]\n",
      "-------------\n",
      "It's dangerous to ignore the signal at a railroad crossing.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[start] es peligroso que [UNK] a la [UNK] del menos me [UNK] [end]\n",
      "-------------\n",
      "Tom ate the whole pizza by himself.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] tom se comió la solo para ti [end]\n",
      "-------------\n",
      "Tom helps Mary because he wants to, not because he has to.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[start] tom le pidió a mary que no le [UNK] porque no lo puedo [end]\n",
      "-------------\n",
      "Tell me who you gave your old toolbox to.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[start] dime que le [UNK] que tu otro [UNK] [end]\n",
      "-------------\n",
      "I ate a hot dog for lunch.\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[start] me tomó un perro que [UNK] [end]\n",
      "-------------\n",
      "Stop saying that!\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] deja de decir eso [end]\n",
      "-------------\n",
      "Is your dog mean?\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[start] tu perro es tu [end]\n",
      "-------------\n",
      "The recent advances in medicine are remarkable.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] la [UNK] en los [UNK] son [UNK] [end]\n",
      "-------------\n",
      "I thought it would be best if you told Tom yourself.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[start] pensé que sería mejor que te [UNK] si tom se lo [UNK] a ti mismo [end]\n",
      "-------------\n",
      "We want you to sing a song.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[start] queremos que una canción [end]\n",
      "-------------\n",
      "Why should I help you?\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[start] por qué debería ayudarte [end]\n",
      "-------------\n",
      "I want to sign the contracts.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] quiero [UNK] la [UNK] [end]\n",
      "-------------\n",
      "Tom felt sad.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[start] tom se sentía muy triste [end]\n",
      "-------------\n",
      "He lives there alone.\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] Él vive allí solo [end]\n",
      "-------------\n",
      "He's getting up early.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[start] se está temprano [end]\n",
      "-------------\n",
      "Let's take a 10-minute break.\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[start] [UNK] un las [UNK] [end]\n",
      "-------------\n",
      "She looked very beautiful in her new dress.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] ella se veía muy [UNK] en su nuevo se [UNK] [end]\n",
      "-------------\n",
      "I need to stretch my legs.\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[start] necesito mis [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary() # Getting the vocabulary\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)),spa_vocab)) # Creating a dictionary to be used to retrieve token and the corresponding words\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence]) # Vectorizing the input sentence\n",
    "    decoded_sentence = \"[start]\" # Initializing and defining the first token of the output sentence\n",
    "    for i in range(max_decoded_sentence_length): # Looping till the max sentence length we want\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])  # Vectorizing the previous tokens present in the output sentence\n",
    "        next_token_prediction = seq2seq_rnn.predict([tokenized_input_sentence,tokenized_target_sentence]) # Predicting the next token based on the previous token vectors\n",
    "        sampled_token_index = np.argmax(next_token_prediction[0,i,:]) # Taking the highest predicted word token from the model prediction\n",
    "        sampled_token = spa_index_lookup[sampled_token_index] # Going through the target sequence vocabulary to check which word corresponds to the predicted token\n",
    "        decoded_sentence += \" \" + sampled_token # Adding the word to the output sentence\n",
    "        if sampled_token == \"[end]\": # If we get [end] token before we reach the max sentence length, then break the loop\n",
    "            break\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs] # Taking the english sentences from the test pairs\n",
    "for _ in range(20): # 20 sentences to be predicted\n",
    "    input_sequence = random.choice(test_eng_texts) # Randomly choosing the sentences\n",
    "    print(\"-------------\")\n",
    "    print(input_sequence)\n",
    "    print(decode_sequence(input_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer model (More preferred than RNNs since RNNs are less efficient when treating long sentences, but transformers prove to be efficient in them, thus leading to long document analysis possibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MultiHeadAttention,Dense,LayerNormalization,Input,Embedding,Dropout\n",
    "from keras import Sequential\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderClass(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.attention_layer1 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.attention_layer2 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_block = Sequential([\n",
    "            Dense(units=dense_dim,activation=\"relu\"),\n",
    "            Dense(units=embed_dim)\n",
    "        ])\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "        self.layer_norm3 = LayerNormalization()\n",
    "        self.supports_masking = True # Ensures that layer will propogate its input mask to its output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    # Causal padding ensures that during self-attention calculations in the transformer, each token only attends to the previous tokens in the sequence, not the future ones.\n",
    "    def get_causal_attention_mask(self,inputs): # Causual padding implementation ( Since the transformer model has access to the whole sequence , so that it doesn't directly copy while predicting the N+1 token, we pad the future elements in the sequence)\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size , sequence_length = input_shape[0] , input_shape[1]\n",
    "        # These lines generate two tensors i and j, where i represents a range from 0 to sequence_length - 1 along columns and j represents the same range along rows.\n",
    "        i = tf.range(sequence_length)[:,tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j,dtype=\"int32\") # This line creates a mask where each element is 1 if the corresponding element in i is greater than or equal to the corresponding element in j, and 0 otherwise. \n",
    "        # This ensures that each token only attends to itself and the previous tokens, not the future ones.\n",
    "        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1])) # Reshaping done so that the mask has the correct dimensions to be compatible with the subsequent tiling operation and matches the shape expected by the attention mechanism in the transformer model.\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size,-1), # This line helps us determine how many times the mask matrix will be repeated during tiling\n",
    "             tf.constant([1,1],dtype=\"int32\")],axis=0) # Here [1,1] means that the mask matrix will not be tilled in other dimensions\n",
    "        return tf.tile(mask,mult) # Tiling process means copying the mask matrix for different dimensions(here only 1 dimension whose number of times to be replicated depends on the batch size)\n",
    "    \n",
    "    def call(self,inputs,encoder_outputs,mask=None): # inputs is the target sequence provided to decoder as input, encoder_inputs is the representation of the source sequence of the encoder\n",
    "        causal_mask = self.get_causal_attention_mask(inputs) # Retreiving the causal mask\n",
    "        \n",
    "        # If a padding mask is provided, it's first cast to an integer type and expanded to match the shape of the causal mask. \n",
    "        #Then, a minimum operation is performed element-wise between the padding mask and the causal mask. \n",
    "        #This step ensures that the model doesn't attend to the padded elements during the attention calculation.\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,tf.newaxis,:],dtype=\"int32\") # Preparing the input mask which describes the padding locations in the target sequence\n",
    "            padding_mask = tf.minimum(padding_mask,causal_mask) # Merging the masks together\n",
    "        \n",
    "        # Attention layer 1 has only the inputs sent to the decoder, so the inputs will be the query, key and value for the layer\n",
    "        # Causal mask only applied here because the model only has the source sequence \n",
    "        attention_output_1 = self.attention_layer1(query=inputs,key=inputs,value=inputs,attention_mask=causal_mask) # Pass the causal mask to the first attention layer, which performs self attention over target sequence        \n",
    "        attention_output_1 = self.layer_norm1(inputs + attention_output_1) # Applying layer normalization and residual connection\n",
    "        \n",
    "        # Attention layer 2 has the attention scores and outputs from the previous attention layer which will be the query here, and the outputs sent by the encoder will be the value and key here ( since we are using context information from the encoder as the key and corresponding values to predict the next token)\n",
    "        # Padding mask is used since the model has both target and source sequence here\n",
    "        attention_output_2 = self.attention_layer2(query=attention_output_1,key=encoder_outputs,value=encoder_outputs,attention_mask=padding_mask) # Pass the padding mask to the second attention layer, which relates the source sequence to the target sequence\n",
    "        attention_output_2 = self.layer_norm2(attention_output_1 + attention_output_2) # Applying layer normalization and residual connection\n",
    "        \n",
    "        proj_output = self.dense_block(attention_output_2) # Dense layer block\n",
    "        return self.layer_norm3(attention_output_2 + proj_output) # Apply layer normalization and residual connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mask matrix looks like this :\n",
    "\n",
    "Sequence Length = 5\n",
    "\n",
    "   0 1 2 3 4  <-- Token index (j)\n",
    "  +----------\n",
    "0 | 1 0 0 0 0\n",
    "1 | 1 1 0 0 0\n",
    "2 | 1 1 1 0 0\n",
    "3 | 1 1 1 1 0\n",
    "4 | 1 1 1 1 1\n",
    "^\n",
    "|\n",
    "Token index (i)\n",
    "\n",
    "At (0, 0), the value is 1 because the token at index 0 can attend to itself.\n",
    "At (1, 0), the value is 1 because the token at index 1 can attend to the token at index 0.\n",
    "At (2, 3), the value is 0 because the token at index 2 cannot attend to the token at index 3 since 2 < 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Mask Tensor (Shape: (1, 5, 5)):\n",
    "\n",
    "[[1 0 0 0 0]\n",
    " [1 1 0 0 0]\n",
    " [1 1 1 0 0]\n",
    " [1 1 1 1 0]\n",
    " [1 1 1 1 1]]\n",
    "\n",
    "\n",
    "Mult : If batch_size = 3 , tf.expand_dims(batch_size,-1) --> convert this into a tensor [3]\n",
    "tf.constant([1,1],dtype=\"int32\")],axis=0) --> adds [1,1] in axis = 0, \n",
    "there mult becomes : \n",
    "[[3]\n",
    "[1,1]]\n",
    "After concatenation,\n",
    "mult -->\n",
    "\n",
    "[[3]\n",
    "[1]\n",
    "[1]]\n",
    "\n",
    "Tiling Process:\n",
    "We'll tile the mask tensor along the batch dimension according to the tiling multiplier tensor:\n",
    "\n",
    "The first dimension of the multiplier tensor specifies how many times to repeat the mask tensor along the batch dimension. In this case, it's 3, so we'll have three copies of the mask tensor.\n",
    "The other dimensions of the multiplier tensor specify how many times to repeat the mask tensor along other dimensions. In this case, it's [1, 1], so the mask tensor will not be tiled along any other dimensions.\n",
    "Resultant Tiled Mask Tensor (Shape: (3, 5, 5)):\n",
    "Each example in the batch gets the same mask tensor replicated according to the multiplier tensor:\n",
    "\n",
    "Batch 1:\n",
    "[[1 0 0 0 0]\n",
    " [1 1 0 0 0]\n",
    " [1 1 1 0 0]\n",
    " [1 1 1 1 0]\n",
    " [1 1 1 1 1]]\n",
    "\n",
    "Batch 2:\n",
    "[[1 0 0 0 0]\n",
    " [1 1 0 0 0]\n",
    " [1 1 1 0 0]\n",
    " [1 1 1 1 0]\n",
    " [1 1 1 1 1]]\n",
    "\n",
    "Batch 3:\n",
    "[[1 0 0 0 0]\n",
    " [1 1 0 0 0]\n",
    " [1 1 1 0 0]\n",
    " [1 1 1 1 0]\n",
    " [1 1 1 1 1]]\n",
    "\n",
    "Each batch receives the same mask tensor replicated according to the tiling multiplier tensor, ensuring that the same mask is applied to each example independently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API for the transformer model\n",
    "\n",
    "def get_causal_attention_mask(inputs):\n",
    "    input_shape = tf.shape(inputs)\n",
    "    batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "    i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "    j = tf.range(sequence_length)\n",
    "    mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "    mult = tf.concat([tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=\"int32\")], axis=0)\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "def transformer_decoder(num_heads,embed_dim,dense_dim,inputs,encoder_outputs,mask=None):\n",
    "\n",
    "    causal_mask = get_causal_attention_mask(inputs)\n",
    "\n",
    "    if mask is not None:\n",
    "\n",
    "        padding_mask = tf.cast(mask[:,tf.newaxis,:],dtype=\"int32\")\n",
    "        padding_mask = tf.minimum(padding_mask,causal_mask)\n",
    "\n",
    "    attention_layer_1 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "    attention_output_1 = attention_layer_1(query=inputs,key=inputs,value=inputs,attention_mask=causal_mask)\n",
    "    attention_output_1 = LayerNormalization()(attention_output_1 + inputs)\n",
    "\n",
    "    attention_layer_2 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "    attention_output_2 = attention_layer_2(query=attention_output_1,key=encoder_outputs,value=encoder_outputs,attention_mask=padding_mask)\n",
    "    attention_output_2 = LayerNormalization()(attention_output_1 + attention_output_2)\n",
    "\n",
    "    dense_block = Sequential([\n",
    "        Dense(dense_dim,activation=\"relu\"),\n",
    "        Dense(embed_dim)\n",
    "    ])\n",
    "\n",
    "    proj_output = dense_block(attention_output_2)\n",
    "\n",
    "    output_layer = LayerNormalization()(proj_output + attention_output_2)\n",
    "\n",
    "    model_functional = Model(inputs = [inputs,encoder_outputs,mask],outputs=output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim,**kwargs): # The sequence length needs to be known because we need to use that as input dimension for the Positional embedding\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.token_embeddings = Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "        self.positional_embeddings = Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1] # Retreiving the length of the sequence\n",
    "        positions = tf.range(start=0,limit=length,delta=1) # List of number positions (1,2,3,4.....length of the sentence)\n",
    "        embedded_tokens = self.token_embeddings(inputs) # Word embeddings\n",
    "        embedded_positions = self.positional_embeddings(positions) # Position embeddings\n",
    "        return embedded_tokens + embedded_positions # Adding word and position embeddings\n",
    "\n",
    "    def compute_mask(self,inputs,mask=None): # Creating a mask to be able to ignore the zero paddings\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "\n",
    "    def get_config(self): # Created so that we can use this custom class later as a layer\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return config   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass API for Transformer encoder\n",
    "class TransformerEncoderClass(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim # Size of input token vectors\n",
    "        self.dense_dim = dense_dim # Size of dense layer\n",
    "        self.num_heads = num_heads # Number of heads in multi head attention mechanism\n",
    "        self.attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) # Multi head attention layer\n",
    "        # Define the Sequential layers with input_shape\n",
    "        # Output of the multihead attention mechanism will be fed to Dense block\n",
    "        self.dense_block = Sequential([ \n",
    "            Dense(units=dense_dim, activation=\"relu\", input_shape=(None, embed_dim)),  # input_shape should match the output shape of attention_layer\n",
    "            Dense(units=embed_dim)\n",
    "        ])\n",
    "        # Using Layer Normalization instead of Batch Normalization because batch normalization does not work properly with sequence data\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None): # Call is used to call the class\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :] # Converting the 2D mask generated by the embedding layer to 3D or 4D as required by the attention layer\n",
    "\n",
    "        # Calling the attention layer to display the outputs (the standard Transformer architecture primarily uses the input sequence twice for self-attention (once for queries and once for keys and values))\n",
    "        attention_output = self.attention_layer(inputs, inputs, attention_mask=mask)\n",
    "\n",
    "        proj_input = self.layer_norm1(inputs + attention_output) # Residual connection between the inputs and output of the attention mechanism layer\n",
    "\n",
    "        proj_output = self.dense_block(proj_input) # Calling the dense block to display the outputs\n",
    "\n",
    "        return self.layer_norm2(proj_input + proj_output) # Residual connection between the inputs and output of the Dense block layer\n",
    "\n",
    "     # Implomenting serialization so that we can save the model(Always include when building custom layers)    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " spanish (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_21 (Posit  (None, None, 256)   3845120     ['english[0][0]']                \n",
      " ionalEmbedding)                                                                                  \n",
      "                                                                                                  \n",
      " positional_embedding_22 (Posit  (None, None, 256)   3845120     ['spanish[0][0]']                \n",
      " ionalEmbedding)                                                                                  \n",
      "                                                                                                  \n",
      " transformer_encoder_class_10 (  (None, None, 256)   3155456     ['positional_embedding_21[0][0]']\n",
      " TransformerEncoderClass)                                                                         \n",
      "                                                                                                  \n",
      " transformer_decoder_class_10 (  (None, None, 256)   5259520     ['positional_embedding_22[0][0]',\n",
      " TransformerDecoderClass)                                         'transformer_encoder_class_10[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, None, 256)    0           ['transformer_decoder_class_10[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " dense_51 (Dense)               (None, None, 15000)  3855000     ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,960,216\n",
      "Trainable params: 19,960,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now building the full end to end model using the Transformer Subclass API we build earlier\n",
    "\n",
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "# Encoder section\n",
    "encoder_inputs = Input(shape=(None,),dtype=\"int64\",name=\"english\")\n",
    "positional_embedding_layer1 = PositionalEmbedding(sequence_length=sequence_length,input_dim=vocab_size,output_dim=embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoderClass(embed_dim=embed_dim,dense_dim=dense_dim,num_heads=num_heads)(positional_embedding_layer1)\n",
    "\n",
    "# Decoder section\n",
    "decoder_inputs = Input(shape=(None,),dtype=\"int64\",name=\"spanish\")\n",
    "positional_embedding_layer2 = PositionalEmbedding(sequence_length=sequence_length,input_dim=vocab_size,output_dim=embed_dim)(decoder_inputs)\n",
    "decoder_outputs = TransformerDecoderClass(embed_dim=embed_dim,dense_dim=dense_dim,num_heads=num_heads)(positional_embedding_layer2,encoder_outputs) # Passing the encoder output here as input along with the previous embedding layer\n",
    "dropout_layer = Dropout(0.5)(decoder_outputs)\n",
    "\n",
    "# Dense layer to convert it into probability scores\n",
    "dense_layer = Dense(vocab_size,activation=\"softmax\")(dropout_layer) # Predicting a word for each output position\n",
    "\n",
    "transformer_encoder_decoder_model = Model([encoder_inputs,decoder_inputs],dense_layer)\n",
    "transformer_encoder_decoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302/1302 [==============================] - 69s 51ms/step - loss: 1.7330 - accuracy: 0.3953 - val_loss: 1.4166 - val_accuracy: 0.4799\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 70s 54ms/step - loss: 1.3803 - accuracy: 0.5138 - val_loss: 1.2143 - val_accuracy: 0.5502\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 75s 58ms/step - loss: 1.2187 - accuracy: 0.5684 - val_loss: 1.1214 - val_accuracy: 0.5871\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 1.1253 - accuracy: 0.6031 - val_loss: 1.0663 - val_accuracy: 0.6150\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 76s 58ms/step - loss: 1.0747 - accuracy: 0.6261 - val_loss: 1.0379 - val_accuracy: 0.6291\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 76s 59ms/step - loss: 1.0438 - accuracy: 0.6419 - val_loss: 1.0278 - val_accuracy: 0.6347\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 76s 59ms/step - loss: 1.0213 - accuracy: 0.6549 - val_loss: 1.0151 - val_accuracy: 0.6422\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 77s 59ms/step - loss: 1.0044 - accuracy: 0.6649 - val_loss: 1.0124 - val_accuracy: 0.6436\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 0.9890 - accuracy: 0.6735 - val_loss: 1.0140 - val_accuracy: 0.6457\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 0.9744 - accuracy: 0.6810 - val_loss: 1.0106 - val_accuracy: 0.6513\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 78s 60ms/step - loss: 0.9619 - accuracy: 0.6878 - val_loss: 1.0148 - val_accuracy: 0.6520\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 80s 62ms/step - loss: 0.9501 - accuracy: 0.6938 - val_loss: 1.0091 - val_accuracy: 0.6546\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 88s 67ms/step - loss: 0.9376 - accuracy: 0.6994 - val_loss: 1.0167 - val_accuracy: 0.6538\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 83s 64ms/step - loss: 0.9261 - accuracy: 0.7046 - val_loss: 1.0148 - val_accuracy: 0.6540\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 84s 65ms/step - loss: 0.9134 - accuracy: 0.7099 - val_loss: 1.0163 - val_accuracy: 0.6571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f11a654e20>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder_decoder_model.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=\"accuracy\")\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath=\"transformer_encoder_decoder\",save_best_only=True)]\n",
    "\n",
    "transformer_encoder_decoder_model.fit(train_dataset,epochs=15,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "I must get this work done by the day after tomorrow.\n",
      "[start] debo llegar a este trabajo mañana [end]\n",
      "-\n",
      "Lunch is ready.\n",
      "[start] el almuerzo está listo [end]\n",
      "-\n",
      "The people are so friendly.\n",
      "[start] las personas son tan [UNK] [end]\n",
      "-\n",
      "I'm very excited.\n",
      "[start] estoy muy el [UNK] [end]\n",
      "-\n",
      "You know about that, don't you?\n",
      "[start] sabes acerca de eso no [end]\n",
      "-\n",
      "He was standing at the gate.\n",
      "[start] Él estaba de vuelta a la puerta [end]\n",
      "-\n",
      "I want you to meet him in order to hear his opinion.\n",
      "[start] quiero que lo [UNK] en el que no se haga tu opinión [end]\n",
      "-\n",
      "Justice will prevail.\n",
      "[start] la [UNK] se [UNK] [end]\n",
      "-\n",
      "Have your friends deserted you?\n",
      "[start] tus amigos te gustan tus amigos [end]\n",
      "-\n",
      "Tom kept the secret to himself.\n",
      "[start] tom se fue un secreto para sí mismo [end]\n",
      "-\n",
      "There is more money than is needed.\n",
      "[start] hay más dinero de lo que necesito [end]\n",
      "-\n",
      "He's jealous.\n",
      "[start] está la [UNK] [end]\n",
      "-\n",
      "I haven't seen one of these in years.\n",
      "[start] no he visto uno de estos años en tokio [end]\n",
      "-\n",
      "I prefer to go barefoot in the house.\n",
      "[start] prefiero ir a la oscuridad [end]\n",
      "-\n",
      "There's no more salt.\n",
      "[start] no hay más sal [end]\n",
      "-\n",
      "The boy is wearing glasses.\n",
      "[start] el niño tiene [UNK] [end]\n",
      "-\n",
      "I can't explain it either.\n",
      "[start] no puedo [UNK] o no [end]\n",
      "-\n",
      "When do you expect him back?\n",
      "[start] cuándo [UNK] [end]\n",
      "-\n",
      "When will the film be released?\n",
      "[start] cuándo [UNK] la película [end]\n",
      "-\n",
      "He was not just a father to me.\n",
      "[start] no era solo un padre para mí [end]\n"
     ]
    }
   ],
   "source": [
    "# Testing our model\n",
    "\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer_encoder_decoder_model([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
