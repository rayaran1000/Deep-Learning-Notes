1. Tuning key gradient parameters for the model : Required when the training is not getting started for the model , or training loss stalls after some time. Happens when their is an 
issue with the configuration of the gradient descent process - choice of optimzer, initial distribution of weights of the model , learning rate and batch size of the model. 
Remedy - Try to change the gradient descent (reduce or increase - depends on whether the model is not reaching global minima or the training loss reduction is very low) , batch size increase (more samples in batch lead to less noisy gradients), while keeping the remaining parameters constant.
2. Better architecture of the model - If the training reduction is getting reduced, but no improvements in the validation metrics, the model is not able to generalize. Maybe the input data is not containing enough information to predict the target column, or maybe the model is not suited for the prediction.
3. Increasing model capacity / size - Training and Validation loss are going down, the next step is to overfit the model. If the model is not able to overfit, it means the model is not complex or large enough to solve the issue. A model should always overfit, so that we can find the appropriate number of epochs required for best results. So try to increase the size of the model by introducing new layers or increasing neurons of each layers.
4. Improving Model generalization - Once the model is overfitted, the next step is to maximizing the model generalization on new data. 
-   The input data should be less noisy, should be huge in size, with minimized label errors and cleaned as much as possible, and should have the essential features.
-   Feature engineering is the major step required to maximize generalization
-   Using early stopping to stop the training of the model to a point where the validation score is not improving, but the training score is improving : stop at that time to prevent overfit
-   Regularization : Prevent the model fitting completely on training data , with the goal of improving validation scores. Few steps are : Reducing network size, Adding weight regularizations like l1 and l2 (for smaller networks) and dropout for larger networks.
