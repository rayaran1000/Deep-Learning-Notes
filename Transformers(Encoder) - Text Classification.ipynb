{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention mechanism steps : 1. Calculating the attention scores by dot product of the target word vector with each word vector present in the sentence\n",
    "# 2. Calculating the sum of all the word vectors present in the sentence by their relevance score(attention scores). This will create our new target word vector, which will also contain \n",
    "# information about the surrounding of the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention mechanism\n",
    "# Steps performed : Step 1: Initialing the output array\n",
    "# Step 2 : Iterating through the input sequence so each word can once be the target word\n",
    "# Step 3 : \"scores\" is the array that will contain the attention scores for target word with each word vector present in the sequence - Initialized \n",
    "# Step 4 : If input sequence is [[1,2,3],[4,5,6],[7,8,9]] , then\n",
    "#For the first token [1, 2, 3]: -- Target token\n",
    "#Dot product with itself: 1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "#Dot product with the second token [4, 5, 6]: 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n",
    "#Dot product with the third token [7, 8, 9]: 1*7 + 2*8 + 3*9 = 7 + 16 + 27 = 50\n",
    "#So, the attention scores for the first token [1, 2, 3] are [14, 32, 50].\n",
    "# Step 5 : Once we get the attention scores, we scale them using np.sqrt and normalize them using softmax activation\n",
    "# Step 6 : We get the weighted vectors of the attention scores as the new vector representation\n",
    "# Target token --> [1,2,3]\n",
    "# Attention scores --> [14,32,50] --> sum = 14 + 32 + 50 = 96   \n",
    "# Therefore, weighted vector_1 of 1st token --> [14/96,32/96,50/96] => [1/6, 1/3, 1/2]\n",
    "# Similarly, weighted vector_2 of 1st token --> [8/24, 10/24, 12/24]\n",
    "# Similarly, weighted vector_3 of 1st token --> [35/96, 40/96, 45/96]\n",
    "\n",
    "# Therfore, the new pivot representation will be [1/6 + 8/24 + 35/96, 1/3 + 10/24 + 40/96, 1/2 + 12/24 + 45/96]  -> For 1st token\n",
    "\n",
    "# Similarly, the new pivot representation will be calculated for the next word in the input sequence as the target token\n",
    "# Input sequence is a sequuence with vector representation of the words\n",
    "def self_attention(input_sequence):\n",
    "    output = np.zeros(shape=input_sequence) # Initializing the output\n",
    "    for i , pivot_vector in enumerate(input_sequence): # pivot_vector is each token in the sentence ( Each token in the sentence will be a target token once)\n",
    "        scores = np.zeros(shape=(len(input_sequence),)) # Initializing the scores (Depends on the number of tokens in input_sequence)\n",
    "        for j,vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector,vector.T) # Used for computing the attention scores as a dot product between the pivot vector and the remaining vectors (vector.T means transpose )\n",
    "        scores /= np.sqrt(input_sequence.shape[1]) # Scaling \n",
    "        scores = softmax(scores) # Softmax activation function applied\n",
    "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
    "        for j,vector in enumerate(input_sequence):\n",
    "            new_pivot_representation += scores[j] * vector # This vector representation of each word will contain the information about the surrounding words as well\n",
    "        output[i] = new_pivot_representation\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation behind new pivot representation\n",
    "For the first token [1, 2, 3]:\n",
    "\n",
    "Weight for the first vector [1, 2, 3]:\n",
    "\n",
    "    weight_1 = 14 / (14 + 32 + 50 + 68) = 14 / 164 ≈ 0.0854\n",
    "\n",
    "Weight for the second vector [4, 5, 6]:\n",
    "\n",
    "    weight_2 = 32 / (14 + 32 + 50 + 68) = 32 / 164 ≈ 0.1951\n",
    "\n",
    "Weight for the third vector [7, 8, 9]:\n",
    "\n",
    "    weight_3 = 50 / (14 + 32 + 50 + 68) = 50 / 164 ≈ 0.3049\n",
    "\n",
    "Weight for the fourth vector [10, 11, 12]:\n",
    "\n",
    "    weight_4 = 68 / (14 + 32 + 50 + 68) = 68 / 164 ≈ 0.4146\n",
    "\n",
    "Weighted vector for the first vector [1, 2, 3]:\n",
    "\n",
    "    weighted_vector_1 = [0.0854, 0.1708, 0.2562]\n",
    "\n",
    "Weighted vector for the second vector [4, 5, 6]:\n",
    "\n",
    "    weighted_vector_2 = [0.7804, 0.9756, 1.1707]\n",
    "\n",
    "Weighted vector for the third vector [7, 8, 9]:\n",
    "\n",
    "    weighted_vector_3 = [2.1348, 2.4393, 2.7439]\n",
    "\n",
    "Weighted vector for the fourth vector [10, 11, 12]:\n",
    "\n",
    "    weighted_vector_4 = [4.1557, 4.5713, 4.9868]\n",
    "\n",
    "\n",
    "Sum of weighted vectors:\n",
    "\n",
    "    [0.0854 + 0.7804 + 2.1348 + 4.1557, \n",
    "     0.1708 + 0.9756 + 2.4393 + 4.5713,\n",
    "     0.2562 + 1.1707 + 2.7439 + 4.9868]\n",
    "\n",
    "New pivot representation:\n",
    "\n",
    "    [6.4563, 7.6974, 9.9377]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_2 = 32 / (14 + 32 + 50 + 68) = 32 / 164 ≈ 0.1951 \n",
    "\n",
    "Vector_2 = [4 , 5 , 6]\n",
    "\n",
    "Weighted vector for the second vector:\n",
    "[4 * 0.1951, 5 * 0.1951, 6 * 0.1951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In keras, we use a vectorized implementation of the above since its faster, its present in MultiHeadAttention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_heads = 4\n",
    "#embed_dim = 256\n",
    "\n",
    "#mha_layer = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "#outputs = mha_layer(inputs,inputs,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transformer was originally created to translate from source sequence to target sequence ( That's why transformers are called sequence to sequence models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi head attention mechanism : Similar to Seperable Convolutional layers, we split the input query,key and value into different parts, and after applying attention mechanism to them, group them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder : Consists of this Multi headed Attention mechanism , since our model is deep so we are required to add residual connections to prevent loss of information and Normalization to run gradient descent faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Transformer encoder from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LayerNormalization , Dense, Input, MultiHeadAttention\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclass API for Transformer encoder\n",
    "class TransformerEncoderClass(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim # Size of input token vectors\n",
    "        self.dense_dim = dense_dim # Size of dense layer\n",
    "        self.num_heads = num_heads # Number of heads in multi head attention mechanism\n",
    "        self.attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) # Multi head attention layer\n",
    "        # Define the Sequential layers with input_shape\n",
    "        # Output of the multihead attention mechanism will be fed to Dense block\n",
    "        self.dense_block = Sequential([ \n",
    "            Dense(units=dense_dim, activation=\"relu\", input_shape=(None, embed_dim)),  # input_shape should match the output shape of attention_layer\n",
    "            Dense(units=embed_dim)\n",
    "        ])\n",
    "        # Using Layer Normalization instead of Batch Normalization because batch normalization does not work properly with sequence data\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None): # Call is used to call the class\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :] # Converting the 2D mask generated by the embedding layer to 3D or 4D as required by the attention layer\n",
    "\n",
    "        # Calling the attention layer to display the outputs (the standard Transformer architecture primarily uses the input sequence twice for self-attention (once for queries and once for keys and values))\n",
    "        attention_output = self.attention_layer(inputs, inputs, attention_mask=mask)\n",
    "\n",
    "        proj_input = self.layer_norm1(inputs + attention_output) # Residual connection between the inputs and output of the attention mechanism layer\n",
    "\n",
    "        proj_output = self.dense_block(proj_input) # Calling the dense block to display the outputs\n",
    "\n",
    "        return self.layer_norm2(proj_input + proj_output) # Residual connection between the inputs and output of the Dense block layer\n",
    "\n",
    "     # Implomenting serialization so that we can save the model(Always include when building custom layers)    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API for Transformer encoder\n",
    "\n",
    "def TransformerEncoder(embed_dim, dense_dim, num_heads, inputs, mask=None):\n",
    "    # Multi-head attention layer\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "    \n",
    "    # Dense block\n",
    "    dense_block = Sequential([\n",
    "        Dense(units=dense_dim, activation=\"relu\"),\n",
    "        Dense(units=embed_dim)\n",
    "    ])\n",
    "    \n",
    "    # Layer normalization layers\n",
    "    layer_norm1 = LayerNormalization()\n",
    "    layer_norm2 = LayerNormalization()\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        mask = mask[:, tf.newaxis, :]\n",
    "    \n",
    "    # Compute attention output\n",
    "    attention_output = attention_layer(inputs, inputs, attention_mask=mask) \n",
    "    \n",
    "    # Layer normalization and residual connection before dense block\n",
    "    proj_input = layer_norm1(inputs + attention_output)\n",
    "    \n",
    "    # Compute output of the dense block\n",
    "    proj_output = dense_block(proj_input)\n",
    "    \n",
    "    # Layer normalization and residual connection after dense block\n",
    "    final_output = layer_norm2(proj_input + proj_output)\n",
    "    \n",
    "    return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow of the encoder : Word Vector Embeddings get feeded to Multi headed attention mechanism , the output is then added with the original input as part of residual connection, normalization done,\n",
    "# The normalized input is then passed to the Dense block(2 Dense layer) , the output of the Dense layers is then added to the normalized input, then the final result is again normalized and sent as output of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization, Embedding, GlobalMaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "import pathlib\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "dense_dim = 32\n",
    "num_heads = 2\n",
    "\n",
    "input_layer = Input(shape=(None,), dtype=\"int64\")\n",
    "embedding_layer = Embedding(vocab_size, embed_dim)(input_layer)  # Embedding layer for token IDs\n",
    "encoder_layer = TransformerEncoderClass(embed_dim,dense_dim,num_heads)(embedding_layer)\n",
    "max_pool_layer = GlobalMaxPooling1D()(encoder_layer) # Since our encoder returns full sequences and we need to reduce each sequence to a single vector for classification \n",
    "dropout_layer = Dropout(0.5)(max_pool_layer)\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
    "\n",
    "model1 = Model(input_layer, output_layer)\n",
    "model1.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder_class (  (None, None, 256)        543776    \n",
      " TransformerEncoderClass)                                        \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "IMDB_DATADIR = r'C:\\Users\\arany\\.keras\\datasets\\aclimdb'\n",
    "\n",
    "base_dir = pathlib.Path(IMDB_DATADIR)\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "batch_size = 32\n",
    "test_dir = base_dir / \"test\"\n",
    "\n",
    "train_dataset = keras.utils.text_dataset_from_directory(train_dir,batch_size=batch_size)\n",
    "test_dataset = keras.utils.text_dataset_from_directory(test_dir,batch_size=batch_size)\n",
    "validation_dataset = keras.utils.text_dataset_from_directory(val_dir,batch_size=batch_size)\n",
    "\n",
    "text_vectorization = TextVectorization(max_tokens=vocab_size,output_mode=\"int\",output_sequence_length=600) # Output mode set as integers, because sequential models use input sequnces as input(integer tokens representing words)\n",
    "\n",
    "text_only_train_dataset = train_dataset.map(lambda x , y : x)\n",
    "text_vectorization.adapt(text_only_train_dataset)\n",
    "\n",
    "int_train_dataset = train_dataset.map(lambda x,y : (text_vectorization(x) , y) , num_parallel_calls=4)\n",
    "\n",
    "int_test_dataset = test_dataset.map(lambda x,y : (text_vectorization(x), y), num_parallel_calls=4)\n",
    "\n",
    "int_val_dataset = validation_dataset.map(lambda x,y : (text_vectorization(x), y), num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ParallelMapDataset element_spec=(TensorSpec(shape=(None, 600), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4782 - accuracy: 0.7789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 191s 300ms/step - loss: 0.4782 - accuracy: 0.7789 - val_loss: 0.3522 - val_accuracy: 0.8444\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.8670"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 184s 294ms/step - loss: 0.3147 - accuracy: 0.8670 - val_loss: 0.3202 - val_accuracy: 0.8694\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.2518 - accuracy: 0.9002"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 18). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 184s 295ms/step - loss: 0.2518 - accuracy: 0.9002 - val_loss: 0.2717 - val_accuracy: 0.8926\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 179s 286ms/step - loss: 0.2018 - accuracy: 0.9207 - val_loss: 0.2810 - val_accuracy: 0.8872\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 180s 289ms/step - loss: 0.1649 - accuracy: 0.9354 - val_loss: 0.2795 - val_accuracy: 0.8926\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 183s 292ms/step - loss: 0.1321 - accuracy: 0.9508 - val_loss: 0.3135 - val_accuracy: 0.8884\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 228s 364ms/step - loss: 0.1148 - accuracy: 0.9586 - val_loss: 0.3593 - val_accuracy: 0.8692\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 141s 224ms/step - loss: 0.0931 - accuracy: 0.9651 - val_loss: 0.4618 - val_accuracy: 0.8668\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 110s 175ms/step - loss: 0.0770 - accuracy: 0.9729 - val_loss: 0.4439 - val_accuracy: 0.8802\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 109s 174ms/step - loss: 0.0643 - accuracy: 0.9778 - val_loss: 0.5275 - val_accuracy: 0.8758\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 106s 168ms/step - loss: 0.0530 - accuracy: 0.9811 - val_loss: 0.5581 - val_accuracy: 0.8742\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 93s 148ms/step - loss: 0.0465 - accuracy: 0.9832 - val_loss: 0.5797 - val_accuracy: 0.8734\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 102s 162ms/step - loss: 0.0375 - accuracy: 0.9872 - val_loss: 0.6030 - val_accuracy: 0.8706\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 103s 164ms/step - loss: 0.0303 - accuracy: 0.9898 - val_loss: 0.6261 - val_accuracy: 0.8704\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 102s 162ms/step - loss: 0.0254 - accuracy: 0.9915 - val_loss: 0.7928 - val_accuracy: 0.8606\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 100s 160ms/step - loss: 0.0179 - accuracy: 0.9938 - val_loss: 0.8735 - val_accuracy: 0.8588\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 97s 154ms/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 1.1453 - val_accuracy: 0.8442\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 97s 154ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.8410 - val_accuracy: 0.8616\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 97s 154ms/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 1.2556 - val_accuracy: 0.8538\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 96s 153ms/step - loss: 0.0117 - accuracy: 0.9966 - val_loss: 1.1224 - val_accuracy: 0.8584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c702ff9e40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list1 = [ModelCheckpoint(filepath=\"transformer_encoder_model\",save_best_only=True)]\n",
    "\n",
    "history1 = model1.fit(int_train_dataset,validation_data=int_val_dataset,epochs=20,callbacks=callbacks_list1)\n",
    "history1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Transformer encoder class is a custom layer, we are mentioning it here using the \"name\" that we got when we did model.summary()\n",
    "transformer_encoder_model = keras.models.load_model(\"transformer_encoder_model\",custom_objects={\"transformer_encoder_model\" : TransformerEncoderClass})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 66s 81ms/step - loss: 0.2775 - accuracy: 0.8896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.277483731508255, 0.8895599842071533]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder_model.evaluate(int_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transformer model doesn't have word position information, its only taking set of words sequence information and manually using that ( information about surrounding words using attention scores)\n",
    "# In order to implement this word position information in the sequence, we do positional encoding( It takes into factor the position of a word in the sequence and adds that to the vector embedding of the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer now converted to Positional Encoding layer (contains both vector embedding and position encoding) --> Both the encodings will be learned by the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim,**kwargs): # The sequence length needs to be known because we need to use that as input dimension for the Positional embedding\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.token_embeddings = Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "        self.positional_embeddings = Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1] # Retreiving the length of the sequence\n",
    "        positions = tf.range(start=0,limit=length,delta=1) # List of number positions (1,2,3,4.....length of the sentence)\n",
    "        embedded_tokens = self.token_embeddings(inputs) # Word embeddings\n",
    "        embedded_positions = self.positional_embeddings(positions) # Position embeddings\n",
    "        return embedded_tokens + embedded_positions # Adding word and position embeddings\n",
    "\n",
    "    def compute_mask(self,inputs,mask=None): # Creating a mask to be able to ignore the zero paddings\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "\n",
    "    def get_config(self): # Created so that we can use this custom class later as a layer\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return config   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding_2 (Pos  (None, None, 256)        5273600   \n",
      " itionalEmbedding)                                               \n",
      "                                                                 \n",
      " transformer_encoder_class_2  (None, None, 256)        543776    \n",
      "  (TransformerEncoderClass)                                      \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 256)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,817,633\n",
      "Trainable params: 5,817,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now we wll be using this custom Positional Embedding layer and the Transformer Encoder layer classes together to build the Sequence model\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "dense_dim = 32\n",
    "num_heads = 2\n",
    "\n",
    "input_layer = Input(shape=(None,), dtype=\"int64\")\n",
    "positional_embedding_layer = PositionalEmbedding(sequence_length=sequence_length,input_dim=vocab_size,output_dim=embed_dim)(input_layer)\n",
    "encoder_layer = TransformerEncoderClass(embed_dim,dense_dim,num_heads)(positional_embedding_layer)\n",
    "max_pool_layer = GlobalMaxPooling1D()(encoder_layer) # Since our encoder returns full sequences and we need to reduce each sequence to a single vector for classification \n",
    "dropout_layer = Dropout(0.5)(max_pool_layer)\n",
    "output_layer = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
    "\n",
    "model2 = Model(input_layer, output_layer)\n",
    "model2.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.7710"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_5_layer_call_fn, embedding_5_layer_call_and_return_conditional_losses, embedding_6_layer_call_fn, embedding_6_layer_call_and_return_conditional_losses, multi_head_attention_1_layer_call_fn while saving (showing 5 of 22). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_positional\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_positional\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 158s 221ms/step - loss: 0.4866 - accuracy: 0.7710 - val_loss: 0.3091 - val_accuracy: 0.8634\n",
      "Epoch 2/20\n",
      "625/625 [==============================] - 95s 150ms/step - loss: 0.2460 - accuracy: 0.9025 - val_loss: 0.3236 - val_accuracy: 0.8668\n",
      "Epoch 3/20\n",
      "625/625 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_5_layer_call_fn, embedding_5_layer_call_and_return_conditional_losses, embedding_6_layer_call_fn, embedding_6_layer_call_and_return_conditional_losses, multi_head_attention_1_layer_call_fn while saving (showing 5 of 22). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_positional\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: transformer_encoder_positional\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 110s 176ms/step - loss: 0.1823 - accuracy: 0.9318 - val_loss: 0.2758 - val_accuracy: 0.8906\n",
      "Epoch 4/20\n",
      "625/625 [==============================] - 94s 149ms/step - loss: 0.1508 - accuracy: 0.9463 - val_loss: 0.3345 - val_accuracy: 0.8822\n",
      "Epoch 5/20\n",
      "625/625 [==============================] - 85s 135ms/step - loss: 0.1260 - accuracy: 0.9532 - val_loss: 0.3262 - val_accuracy: 0.8900\n",
      "Epoch 6/20\n",
      "625/625 [==============================] - 92s 146ms/step - loss: 0.1093 - accuracy: 0.9625 - val_loss: 0.3281 - val_accuracy: 0.8834\n",
      "Epoch 7/20\n",
      "625/625 [==============================] - 103s 164ms/step - loss: 0.0977 - accuracy: 0.9661 - val_loss: 0.3876 - val_accuracy: 0.8846\n",
      "Epoch 8/20\n",
      "625/625 [==============================] - 97s 155ms/step - loss: 0.0870 - accuracy: 0.9704 - val_loss: 0.3851 - val_accuracy: 0.8782\n",
      "Epoch 9/20\n",
      "625/625 [==============================] - 96s 153ms/step - loss: 0.0754 - accuracy: 0.9736 - val_loss: 0.4502 - val_accuracy: 0.8818\n",
      "Epoch 10/20\n",
      "625/625 [==============================] - 94s 149ms/step - loss: 0.0709 - accuracy: 0.9774 - val_loss: 0.4796 - val_accuracy: 0.8792\n",
      "Epoch 11/20\n",
      "625/625 [==============================] - 93s 148ms/step - loss: 0.0599 - accuracy: 0.9793 - val_loss: 0.4411 - val_accuracy: 0.8740\n",
      "Epoch 12/20\n",
      "625/625 [==============================] - 94s 149ms/step - loss: 0.0544 - accuracy: 0.9815 - val_loss: 0.6102 - val_accuracy: 0.8682\n",
      "Epoch 13/20\n",
      "625/625 [==============================] - 95s 151ms/step - loss: 0.0471 - accuracy: 0.9834 - val_loss: 0.6046 - val_accuracy: 0.8724\n",
      "Epoch 14/20\n",
      "625/625 [==============================] - 92s 146ms/step - loss: 0.0431 - accuracy: 0.9856 - val_loss: 0.9501 - val_accuracy: 0.8384\n",
      "Epoch 15/20\n",
      "625/625 [==============================] - 93s 148ms/step - loss: 0.0380 - accuracy: 0.9876 - val_loss: 0.6818 - val_accuracy: 0.8730\n",
      "Epoch 16/20\n",
      "625/625 [==============================] - 93s 147ms/step - loss: 0.0341 - accuracy: 0.9895 - val_loss: 0.9000 - val_accuracy: 0.8664\n",
      "Epoch 17/20\n",
      "625/625 [==============================] - 92s 146ms/step - loss: 0.0319 - accuracy: 0.9900 - val_loss: 0.7518 - val_accuracy: 0.8706\n",
      "Epoch 18/20\n",
      "625/625 [==============================] - 91s 145ms/step - loss: 0.0295 - accuracy: 0.9905 - val_loss: 0.7613 - val_accuracy: 0.8630\n",
      "Epoch 19/20\n",
      "625/625 [==============================] - 92s 147ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.8236 - val_accuracy: 0.8724\n",
      "Epoch 20/20\n",
      "625/625 [==============================] - 91s 145ms/step - loss: 0.0267 - accuracy: 0.9905 - val_loss: 0.8103 - val_accuracy: 0.8630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c70382a920>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list2 = [ModelCheckpoint(filepath=\"transformer_encoder_positional\",save_best_only=True,monitor=\"val_loss\")]\n",
    "history2 = model2.fit(int_train_dataset,validation_data = int_val_dataset,epochs=20,callbacks=callbacks_list2)\n",
    "history2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the Transformer encoder class is a custom layer, we are mentioning it here using the \"name\" that we got when we did model.summary()\n",
    "transformer_encoder_positional_model = keras.models.load_model(\"transformer_encoder_model\",custom_objects={\"transformer_encoder_model\" : TransformerEncoderClass, \"positional_embedding\" : PositionalEmbedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 62s 76ms/step - loss: 0.2775 - accuracy: 0.8896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.27748382091522217, 0.8895599842071533]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder_positional_model.evaluate(int_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
