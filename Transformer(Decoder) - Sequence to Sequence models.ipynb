{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine translation use case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset download\n",
    "\n",
    "dataset = tf.keras.utils.get_file(fname=\"spa-eng.zip\",origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\", extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "DATASET_DIR = r\"C:\\Users\\arany\\.keras\\datasets\\spa-eng\"\n",
    "base_path = pathlib.Path(DATASET_DIR)\n",
    "\n",
    "text_filepath = base_path / \"spa.txt\"\n",
    "with open(text_filepath) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "\n",
    "for line in lines:\n",
    "    english , spanish = line.split(\"\\t\") # Since each line contains the english and spanish sentence as tab seperated \n",
    "    spanish = \"[start] \" + spanish + \" [end]\" # So that we get a start and end sentence tokens for each spanish word (used in decoder)\n",
    "    text_pairs.append((english,spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Turn off the gas.', '[start] Corta el gas. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Test and Validation split\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = int(len(text_pairs) - 2*num_val_samples)\n",
    "num_test_samples = num_train_samples - num_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples+ num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create 2 seperate text vectorization for 2 different language(punctuations may be different) ( Also the brackets might get removed from spanish translation in [start] and [end], but we need them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"Â¿\" # Adding the character as punctuation\n",
    "strip_chars = strip_chars.replace(\"[\",\"\") # Removing brackets from string punctuations\n",
    "strip_chars = strip_chars.replace(\"]\",\"\")\n",
    "\n",
    "def custom_standardization(input_string): # Function for lower case conversioon and removing and adding punctuation for the spanish language\n",
    "    output_string = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(output_string,f\"[{re.escape(strip_chars)}]\",\"\") # Removing the punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000 # top 15000 frequent words\n",
    "sequence_length = 20 # Sequence length restricted to 20 words\n",
    "\n",
    "# Vectorization of the source sequence(english) and the target sequence(spanish)\n",
    "\n",
    "source_vectorization = TextVectorization(max_tokens=vocab_size,output_mode=\"int\",output_sequence_length=sequence_length)\n",
    "\n",
    "# Generating spanish sequences with 1 extra token per sentence because we need to offset the sentence by one step during training\n",
    "# If both source and target have the same number of tokens, then +1 added in the output sequence length means it will be predicting the next token..i,e the 4th token\n",
    "# If we don't add +1 , then since both the source and target have same length, then there is no new next token to predict( so we add + 1 to the output_sequence_length)\n",
    "target_vectorization = TextVectorization(max_tokens=vocab_size,output_mode=\"int\",output_sequence_length=sequence_length + 1,standardize=custom_standardization) \n",
    "\n",
    "# Extracting the texts seperately and training our vocabularies seperately for english and spanish\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data pipeline should return the below tuple\n",
    "# (inputs,target) where inputs = {\"encoder_inputs\":\"english sentence from the input file\" , \"decoder_inputs\" : \"spanish sentence from the input file\"}\n",
    "# target is the Spanish sentence offset by one step ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the dataset \n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng,spa): # Function to get the tuple\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\" : eng ,\n",
    "        \"spanish\" : spa[:,:-1] # The input spanish sequence doesn't include the last token to keep the inputs and targets of same length\n",
    "    }, spa[:,1:]) # The target spanish sequence is one step ahead(Both are still the same length)\n",
    "\n",
    "def make_dataset(pairs): \n",
    "    eng_texts,spa_texts = zip(*pairs) # Unzips the pairs into separate lists of English and Spanish sentences.\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts,spa_texts)) # Converts the lists into TensorFlow dataset\n",
    "    dataset = dataset.batch(batch_size) # Batches the dataset\n",
    "    dataset = dataset.map(format_dataset,num_parallel_calls=4) # This function formats each pair of sentences into the required format(format defined in format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache() # Prefetches 16 batches of data to speed up training\n",
    "\n",
    "train_dataset = make_dataset(train_pairs)\n",
    "\n",
    "val_dataset = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 20)\n",
      "(64, 20)\n",
      "(64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_dataset.take(1):\n",
    "    print(inputs[\"english\"].shape)\n",
    "    print(inputs[\"spanish\"].shape)\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model : Need two RNN components(encoder and decoder) --> encoder will turn the entire sequence into a single or set of vectors ---> This single or set of vectors will be used as initial state\n",
    "# for the decoder, which will look at elements 0 to N in target sequence and try to predict the N+1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GRU instead of LSTM because it makes things simpler,, since a single state vector is used in GRU as compared to multi set vector in LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GRU, Dense, Input, Embedding, Bidirectional, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " spanish (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, None, 256)    3840000     ['english[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, None, 256)    3840000     ['spanish[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 1024)        7876608     ['embedding_4[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " gru_5 (GRU)                    (None, None, 1024)   3938304     ['embedding_5[0][0]',            \n",
      "                                                                  'bidirectional_2[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, None, 1024)   0           ['gru_5[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, None, 15000)  15375000    ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34,869,912\n",
      "Trainable params: 34,869,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "\n",
    "# Encoder\n",
    "source = Input(shape=(None,),dtype=\"int64\",name=\"english\") # English Source sentence\n",
    "embedding_layer1 = Embedding(input_dim=vocab_size,output_dim=embed_dim,mask_zero=True)(source) # Masking is a critical step needed( Sentences with variable lengths will be padded)\n",
    "encoded_source = Bidirectional(GRU(latent_dim),merge_mode=\"sum\")(embedding_layer1) # Output layer of our encoder(\"sum\" means the forward and backward direction representations will be summed together to get the final output encoded representations)\n",
    "\n",
    "# Decoder\n",
    "past_target = Input(shape=(None,),dtype=\"int64\",name=\"spanish\") # Spanish Target sentence\n",
    "embedding_layer2 = Embedding(input_dim=vocab_size,output_dim=embed_dim,mask_zero=True)(past_target) # Masking is critical here as well\n",
    "decoded_gru = GRU(latent_dim,return_sequences=True) # Specifying the units of the GRU layer and we need the full sequence of outputs generated by the decoder not just the final last output , so return_sequences is set to True\n",
    "\n",
    "# The below layer is the main decoder GRU layer and we are initializing it by passing the information that we got as output from our encoder\n",
    "# So the decoder can use the target sequence to predict new tokens using the context information that was learned by the encoder while it was learning on the source input\n",
    "decoded_gru_initialized = decoded_gru(embedding_layer2,initial_state=encoded_source) # Encoder source sequence set as the initial state for the decoder GRU  \n",
    "dropout_layer = Dropout(0.5)(decoded_gru_initialized)\n",
    "target_next_token_layer = Dense(vocab_size,activation=\"softmax\")(dropout_layer)\n",
    "seq2seq_rnn = Model([source,past_target],target_next_token_layer) # Inputs are source and past_target layers , output layer is target_next_token_layer\n",
    "\n",
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1302/1302 [==============================] - 652s 490ms/step - loss: 1.6282 - accuracy: 0.4195 - val_loss: 1.3128 - val_accuracy: 0.5070\n",
      "Epoch 2/15\n",
      "1302/1302 [==============================] - 603s 463ms/step - loss: 1.3111 - accuracy: 0.5281 - val_loss: 1.1497 - val_accuracy: 0.5686\n",
      "Epoch 3/15\n",
      "1302/1302 [==============================] - 96s 74ms/step - loss: 1.1695 - accuracy: 0.5776 - val_loss: 1.0706 - val_accuracy: 0.5998\n",
      "Epoch 4/15\n",
      "1302/1302 [==============================] - 86s 66ms/step - loss: 1.0809 - accuracy: 0.6092 - val_loss: 1.0392 - val_accuracy: 0.6190\n",
      "Epoch 5/15\n",
      "1302/1302 [==============================] - 94s 72ms/step - loss: 1.0323 - accuracy: 0.6339 - val_loss: 1.0231 - val_accuracy: 0.6280\n",
      "Epoch 6/15\n",
      "1302/1302 [==============================] - 94s 72ms/step - loss: 1.0015 - accuracy: 0.6520 - val_loss: 1.0205 - val_accuracy: 0.6353\n",
      "Epoch 7/15\n",
      "1302/1302 [==============================] - 96s 74ms/step - loss: 0.9830 - accuracy: 0.6657 - val_loss: 1.0197 - val_accuracy: 0.6392\n",
      "Epoch 8/15\n",
      "1302/1302 [==============================] - 98s 75ms/step - loss: 0.9691 - accuracy: 0.6761 - val_loss: 1.0247 - val_accuracy: 0.6406\n",
      "Epoch 9/15\n",
      "1302/1302 [==============================] - 98s 75ms/step - loss: 0.9594 - accuracy: 0.6835 - val_loss: 1.0246 - val_accuracy: 0.6421\n",
      "Epoch 10/15\n",
      "1302/1302 [==============================] - 100s 77ms/step - loss: 0.9521 - accuracy: 0.6899 - val_loss: 1.0283 - val_accuracy: 0.6420\n",
      "Epoch 11/15\n",
      "1302/1302 [==============================] - 101s 78ms/step - loss: 0.9466 - accuracy: 0.6937 - val_loss: 1.0310 - val_accuracy: 0.6427\n",
      "Epoch 12/15\n",
      "1302/1302 [==============================] - 103s 79ms/step - loss: 0.9427 - accuracy: 0.6967 - val_loss: 1.0324 - val_accuracy: 0.6437\n",
      "Epoch 13/15\n",
      "1302/1302 [==============================] - 103s 79ms/step - loss: 0.9400 - accuracy: 0.6993 - val_loss: 1.0348 - val_accuracy: 0.6440\n",
      "Epoch 14/15\n",
      "1302/1302 [==============================] - 103s 79ms/step - loss: 0.9379 - accuracy: 0.7007 - val_loss: 1.0362 - val_accuracy: 0.6432\n",
      "Epoch 15/15\n",
      "1302/1302 [==============================] - 105s 81ms/step - loss: 0.9382 - accuracy: 0.7009 - val_loss: 1.0398 - val_accuracy: 0.6432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x173f5c50490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_rnn.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\",metrics=\"accuracy\")\n",
    "seq2seq_rnn.fit(train_dataset,epochs=15,validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Good words are worth a lot, but cost almost nothing.\n",
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "[start] las [UNK] mucho que las mujeres [UNK] mucho pero no son las cosas mÃ¡s tarde [end]\n",
      "-------------\n",
      "We all like cycling.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] a todos nos gusta el extranjero [end]\n",
      "-------------\n",
      "It's dangerous to ignore the signal at a railroad crossing.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[start] es peligroso que [UNK] a la [UNK] del menos me [UNK] [end]\n",
      "-------------\n",
      "Tom ate the whole pizza by himself.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] tom se comiÃ³ la solo para ti [end]\n",
      "-------------\n",
      "Tom helps Mary because he wants to, not because he has to.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[start] tom le pidiÃ³ a mary que no le [UNK] porque no lo puedo [end]\n",
      "-------------\n",
      "Tell me who you gave your old toolbox to.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[start] dime que le [UNK] que tu otro [UNK] [end]\n",
      "-------------\n",
      "I ate a hot dog for lunch.\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "[start] me tomÃ³ un perro que [UNK] [end]\n",
      "-------------\n",
      "Stop saying that!\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] deja de decir eso [end]\n",
      "-------------\n",
      "Is your dog mean?\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[start] tu perro es tu [end]\n",
      "-------------\n",
      "The recent advances in medicine are remarkable.\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] la [UNK] en los [UNK] son [UNK] [end]\n",
      "-------------\n",
      "I thought it would be best if you told Tom yourself.\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "[start] pensÃ© que serÃ­a mejor que te [UNK] si tom se lo [UNK] a ti mismo [end]\n",
      "-------------\n",
      "We want you to sing a song.\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "[start] queremos que una canciÃ³n [end]\n",
      "-------------\n",
      "Why should I help you?\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "[start] por quÃ© deberÃ­a ayudarte [end]\n",
      "-------------\n",
      "I want to sign the contracts.\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "[start] quiero [UNK] la [UNK] [end]\n",
      "-------------\n",
      "Tom felt sad.\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[start] tom se sentÃ­a muy triste [end]\n",
      "-------------\n",
      "He lives there alone.\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] Ãl vive allÃ­ solo [end]\n",
      "-------------\n",
      "He's getting up early.\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "[start] se estÃ¡ temprano [end]\n",
      "-------------\n",
      "Let's take a 10-minute break.\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "[start] [UNK] un las [UNK] [end]\n",
      "-------------\n",
      "She looked very beautiful in her new dress.\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "[start] ella se veÃ­a muy [UNK] en su nuevo se [UNK] [end]\n",
      "-------------\n",
      "I need to stretch my legs.\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[start] necesito mis [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "spa_vocab = target_vectorization.get_vocabulary() # Getting the vocabulary\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)),spa_vocab)) # Creating a dictionary to be used to retrieve token and the corresponding words\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence]) # Vectorizing the input sentence\n",
    "    decoded_sentence = \"[start]\" # Initializing and defining the first token of the output sentence\n",
    "    for i in range(max_decoded_sentence_length): # Looping till the max sentence length we want\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])  # Vectorizing the previous tokens present in the output sentence\n",
    "        next_token_prediction = seq2seq_rnn.predict([tokenized_input_sentence,tokenized_target_sentence]) # Predicting the next token based on the previous token vectors\n",
    "        sampled_token_index = np.argmax(next_token_prediction[0,i,:]) # Taking the highest predicted word token from the model prediction\n",
    "        sampled_token = spa_index_lookup[sampled_token_index] # Going through the target sequence vocabulary to check which word corresponds to the predicted token\n",
    "        decoded_sentence += \" \" + sampled_token # Adding the word to the output sentence\n",
    "        if sampled_token == \"[end]\": # If we get [end] token before we reach the max sentence length, then break the loop\n",
    "            break\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs] # Taking the english sentences from the test pairs\n",
    "for _ in range(20): # 20 sentences to be predicted\n",
    "    input_sequence = random.choice(test_eng_texts) # Randomly choosing the sentences\n",
    "    print(\"-------------\")\n",
    "    print(input_sequence)\n",
    "    print(decode_sequence(input_sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
