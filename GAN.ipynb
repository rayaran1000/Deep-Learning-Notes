{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomness is required while using text generation so that the answers generated by the model are less predictable and more creative\n",
    "# However, there should be a balance in the randomness ( low randomness means predictable boring answers, high randomness means hypothetical creative answers which don't make any sense, so we need a intermediate randomness)\n",
    "# Softmax is used to predict the next word using the probability of the words ( By introducing randomness, if a word has 0.4 , then the word will be chosen 40 percent of the time)\n",
    "# To control the randomness , we use a term called \"softmax temperature\" - 0.0 to 1.0 range --> 0 means low entropy i.e boring answers or no randomness, 1.0 means high entropy i.e hypothetical creative answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How the softmax temperature function works\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reweight_distribution(original_distribution,softmax_temperature=0.5): # Original distribution is 1D numpy array where probabilities sum up to 1 ( probability of words to be the next word in the sentence)\n",
    "\n",
    "    new_distribution = np.log(original_distribution) / softmax_temperature\n",
    "    new_distribution = np.exp(new_distribution)\n",
    "    return new_distribution / np.sum(new_distribution) # Returns a reweighted version of the original distribution( The sum might not be equal to 1, so to make it equal to 1 , we divide by the sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Using IMDB dataset\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Downloading the dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\",\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#                                  origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#                                  extract=True)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m IMDB_DATASET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124marany\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124maclimdb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mtext_dataset_from_directory(directory\u001b[38;5;241m=\u001b[39mIMDB_DATASET,label_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m) \u001b[38;5;66;03m# Since we need a huge amount of data and we dont want any classification tasks, so we dont need the labels and classes(label_mode = None)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x : tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mregex_replace(x,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<br />\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# Using IMDB dataset\n",
    "\n",
    "# Downloading the dataset\n",
    "#dataset = tf.keras.utils.get_file(fname=\"aclImdb_v1.tar.gz\",\n",
    "#                                  origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "#                                  extract=True)\n",
    "IMDB_DATASET = r'C:\\Users\\arany\\.keras\\datasets\\aclimdb'\n",
    "\n",
    "dataset = keras.utils.text_dataset_from_directory(directory=IMDB_DATASET,label_mode=None,batch_size=256) # Since we need a huge amount of data and we dont want any classification tasks, so we dont need the labels and classes(label_mode = None)\n",
    "\n",
    "dataset = dataset.map(lambda x : tf.strings.regex_replace(x,\"<br />\",\" \")) # Removing the <br> html tags present in the reviews(Since we are just generating words, so not needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorization ( creating the vocabulary for text generation)\n",
    "\n",
    "sequence_length = 100\n",
    "vocab_size = 15000 # Top 15000 frequent words are used, else treated as [UNK]\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(dataset) # Using only the text reviews we extracted earlier to adapt our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a layer where the input sequence will the input itself as a tensor, but the target will be offset by 1(since we are training our model for generating the next word)\n",
    "\n",
    "def prepare_lm_dataset(text_batch):\n",
    "    vectorized_sequence = text_vectorization(text_batch) # Converting words to vector\n",
    "    source_sequence = vectorized_sequence[:,:-1] # Creating inputs by cutting off the last word of the sequence\n",
    "    target_sequence = vectorized_sequence[:,1:] # Creating targets by offsetting the sequences by 1\n",
    "    return source_sequence, target_sequence\n",
    "\n",
    "lm_dataset = dataset.map(prepare_lm_dataset,num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we do offsetting and cutting off in inputs and target\n",
    "\n",
    "### Suppose the below matrix representes the vectorization of the words in the sequence\n",
    "\n",
    "vectorized_sequence = [\n",
    "    [0.1, 0.2, 0.3, 0.4],  # The\n",
    "    [0.5, 0.6, 0.7, 0.8],  # cat\n",
    "    [0.9, 1.0, 1.1, 1.2],  # sat\n",
    "    [1.3, 1.4, 1.5, 1.6],  # on\n",
    "    [1.7, 1.8, 1.9, 2.0],  # the\n",
    "    [2.1, 2.2, 2.3, 2.4],  # mat\n",
    "    [0.1, 0.2, 0.3, 0.4],  # The\n",
    "    [2.5, 2.6, 2.7, 2.8],  # dog\n",
    "    [2.9, 3.0, 3.1, 3.2],  # barked\n",
    "    [3.3, 3.4, 3.5, 3.6]   # loudly\n",
    "]\n",
    "\n",
    "source_sequence = [\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6],\n",
    "    [1.7, 1.8, 1.9, 2.0],\n",
    "    [0.1, 0.2, 0.3, 0.4],\n",
    "    [2.5, 2.6, 2.7, 2.8],\n",
    "    [2.9, 3.0, 3.1, 3.2]\n",
    "]\n",
    "\n",
    "target_sequence = [\n",
    "    [0.5, 0.6, 0.7, 0.8],\n",
    "    [0.9, 1.0, 1.1, 1.2],\n",
    "    [1.3, 1.4, 1.5, 1.6],\n",
    "    [1.7, 1.8, 1.9, 2.0],\n",
    "    [2.1, 2.2, 2.3, 2.4],\n",
    "    [2.5, 2.6, 2.7, 2.8],\n",
    "    [2.9, 3.0, 3.1, 3.2],\n",
    "    [3.3, 3.4, 3.5, 3.6]\n",
    "]\n",
    "\n",
    "### So, the line 1 of source_sequence is passed to model as input, the 1st line of target_sequence will be passed as output i.e the next word in the sentence\n",
    "### So each line number 'X' or Xth word being passed from source sequence to model has line number 'X' in target sequence which in turn is the X + 1th word in the sentence\n",
    "### So at each input and target pair, the previous word is the input and the next word is the output\n",
    "### Thats why we offset the target sequence by 1\n",
    "### We cut of the last word in input sequence because there is no next word to be learned by the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use sequence to sequence modelling with the help of encoder and decoder as used previously,, but here in case of text generation, we wont have any source sequence.\n",
    "# We are just trying to predict the next tokens using the past tokens with help of decoder\n",
    "# Causal padding helps in decoder only looking in the 0 to Nth token/ words to predict the N + 1 th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there is no source sequence , we will only be using the decoder part ( decoder only model)\n",
    "# Also we will be using positional embedding from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self,sequence_length,input_dim,output_dim,**kwargs): # The sequence length needs to be known because we need to use that as input dimension for the Positional embedding\n",
    "        super().__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.token_embeddings = Embedding(input_dim=input_dim,output_dim=output_dim)\n",
    "        self.positional_embeddings = Embedding(input_dim=sequence_length,output_dim=output_dim)\n",
    "\n",
    "    def call(self,inputs):\n",
    "        length = tf.shape(inputs)[-1] # Retreiving the length of the sequence\n",
    "        positions = tf.range(start=0,limit=length,delta=1) # List of number positions (1,2,3,4.....length of the sentence)\n",
    "        embedded_tokens = self.token_embeddings(inputs) # Word embeddings\n",
    "        embedded_positions = self.positional_embeddings(positions) # Position embeddings\n",
    "        return embedded_tokens + embedded_positions # Adding word and position embeddings\n",
    "\n",
    "    def compute_mask(self,inputs,mask=None): # Creating a mask to be able to ignore the zero paddings\n",
    "        return tf.math.not_equal(inputs,0)\n",
    "\n",
    "    def get_config(self): # Created so that we can use this custom class later as a layer\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "        })\n",
    "        return config   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderClass(tf.keras.layers.Layer):\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.attention_layer1 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.attention_layer2 = MultiHeadAttention(num_heads=num_heads,key_dim=embed_dim)\n",
    "        self.dense_block = Sequential([\n",
    "            Dense(units=dense_dim,activation=\"relu\"),\n",
    "            Dense(units=embed_dim)\n",
    "        ])\n",
    "        self.layer_norm1 = LayerNormalization()\n",
    "        self.layer_norm2 = LayerNormalization()\n",
    "        self.layer_norm3 = LayerNormalization()\n",
    "        self.supports_masking = True # Ensures that layer will propogate its input mask to its output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"num_heads\": self.num_heads\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    # Causal padding ensures that during self-attention calculations in the transformer, each token only attends to the previous tokens in the sequence, not the future ones.\n",
    "    def get_causal_attention_mask(self,inputs): # Causual padding implementation ( Since the transformer model has access to the whole sequence , so that it doesn't directly copy while predicting the N+1 token, we pad the future elements in the sequence)\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size , sequence_length = input_shape[0] , input_shape[1]\n",
    "        # These lines generate two tensors i and j, where i represents a range from 0 to sequence_length - 1 along columns and j represents the same range along rows.\n",
    "        i = tf.range(sequence_length)[:,tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j,dtype=\"int32\") # This line creates a mask where each element is 1 if the corresponding element in i is greater than or equal to the corresponding element in j, and 0 otherwise. \n",
    "        # This ensures that each token only attends to itself and the previous tokens, not the future ones.\n",
    "        mask = tf.reshape(mask,(1,input_shape[1],input_shape[1])) # Reshaping done so that the mask has the correct dimensions to be compatible with the subsequent tiling operation and matches the shape expected by the attention mechanism in the transformer model.\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size,-1), # This line helps us determine how many times the mask matrix will be repeated during tiling\n",
    "             tf.constant([1,1],dtype=\"int32\")],axis=0) # Here [1,1] means that the mask matrix will not be tilled in other dimensions\n",
    "        return tf.tile(mask,mult) # Tiling process means copying the mask matrix for different dimensions(here only 1 dimension whose number of times to be replicated depends on the batch size)\n",
    "    \n",
    "    def call(self,inputs,encoder_outputs,mask=None): # inputs is the target sequence provided to decoder as input, encoder_inputs is the representation of the source sequence of the encoder\n",
    "        causal_mask = self.get_causal_attention_mask(inputs) # Retreiving the causal mask\n",
    "        \n",
    "        # If a padding mask is provided, it's first cast to an integer type and expanded to match the shape of the causal mask. \n",
    "        #Then, a minimum operation is performed element-wise between the padding mask and the causal mask. \n",
    "        #This step ensures that the model doesn't attend to the padded elements during the attention calculation.\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:,tf.newaxis,:],dtype=\"int32\") # Preparing the input mask which describes the padding locations in the target sequence\n",
    "            padding_mask = tf.minimum(padding_mask,causal_mask) # Merging the masks together\n",
    "        \n",
    "        # Attention layer 1 has only the inputs sent to the decoder, so the inputs will be the query, key and value for the layer\n",
    "        # Causal mask only applied here because the model only has the source sequence \n",
    "        attention_output_1 = self.attention_layer1(query=inputs,key=inputs,value=inputs,attention_mask=causal_mask) # Pass the causal mask to the first attention layer, which performs self attention over target sequence        \n",
    "        attention_output_1 = self.layer_norm1(inputs + attention_output_1) # Applying layer normalization and residual connection\n",
    "        \n",
    "        # Attention layer 2 has the attention scores and outputs from the previous attention layer which will be the query here, and the outputs sent by the encoder will be the value and key here ( since we are using context information from the encoder as the key and corresponding values to predict the next token)\n",
    "        # Padding mask is used since the model has both target and source sequence here\n",
    "        attention_output_2 = self.attention_layer2(query=attention_output_1,key=encoder_outputs,value=encoder_outputs,attention_mask=padding_mask) # Pass the padding mask to the second attention layer, which relates the source sequence to the target sequence\n",
    "        attention_output_2 = self.layer_norm2(attention_output_1 + attention_output_2) # Applying layer normalization and residual connection\n",
    "        \n",
    "        proj_output = self.dense_block(attention_output_2) # Dense layer block\n",
    "        return self.layer_norm3(attention_output_2 + proj_output) # Apply layer normalization and residual connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Dense,Embedding,MultiHeadAttention,LayerNormalization\n",
    "from keras.models import Model\n",
    "from keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 1024\n",
    "num_heads = 2\n",
    "\n",
    "input_layer = Input(shape=(None,),dtype=\"int64\")\n",
    "positional_embedding_layer = PositionalEmbedding(sequence_length=sequence_length,input_dim=vocab_size,output_dim=embed_dim)(input_layer)\n",
    "decoder_layer = TransformerDecoderClass(embed_dim=embed_dim,dense_dim=dense_dim,num_heads=num_heads)(positional_embedding_layer,positional_embedding_layer) # Passing the positional embedding layer twice in the decoder layer is necessary for allowing the self-attention mechanism to consider both the input tokens and their positional information when generating the output sequence in your text generation model\n",
    "# Since we don't have a source sequence and an encoder, so we need to get both the positional information and the previous tokens both from positional embedding\n",
    "output_layer = Dense(vocab_size,activation=\"softmax\")(decoder_layer) # Vocab_size and softmax activation so that we get a probability distribution of each word present in the vocabulary that it can be the next token\n",
    "\n",
    "decoder_model = Model(input_layer,decoder_layer)\n",
    "decoder_model.compile(optimizer=\"rmsprop\",loss=\"sparse_categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tokens_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mtext_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)) \u001b[38;5;66;03m# We are creating a dictionary that maps the word indices to strings(used for text decoding)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_next\u001b[39m(predictions, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m): \u001b[38;5;66;03m# Function for implementing variable temperature sampling from a probability distribution ( the first code block in the notebook)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(predictions)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:487\u001b[0m, in \u001b[0;36mTextVectorization.get_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocabulary\u001b[39m(\u001b[38;5;28mself\u001b[39m, include_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    479\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the current vocabulary of the layer.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03m        returned vocabulary will not include any padding or OOV tokens.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lookup_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:385\u001b[0m, in \u001b[0;36mIndexLookup.get_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    382\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mexport()\n\u001b[0;32m    383\u001b[0m     vocab, indices \u001b[38;5;241m=\u001b[39m (values, keys) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvert \u001b[38;5;28;01melse\u001b[39;00m (keys, values)\n\u001b[0;32m    384\u001b[0m     vocab, indices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_vocab_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    386\u001b[0m         indices\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m    387\u001b[0m     )\n\u001b[0;32m    388\u001b[0m lookup \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moov_token, \u001b[38;5;28mzip\u001b[39m(indices, vocab)\n\u001b[0;32m    390\u001b[0m )\n\u001b[0;32m    391\u001b[0m vocab \u001b[38;5;241m=\u001b[39m [lookup[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_size())]\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:416\u001b[0m, in \u001b[0;36mStringLookup._tensor_vocab_to_numpy\u001b[1;34m(self, vocabulary)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tensor_vocab_to_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    414\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m--> 416\u001b[0m         [tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_text(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocabulary]\n\u001b[0;32m    417\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tensor_vocab_to_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    414\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m--> 416\u001b[0m         [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocabulary]\n\u001b[0;32m    417\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arany\\anaconda3\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\util\\compat.py:110\u001b[0m, in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m bytes_or_text\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bytes_or_text, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m--> 110\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbytes_or_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected binary or unicode string, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m bytes_or_text)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens_index = dict(enumerate(text_vectorization.get_vocabulary())) # We are creating a dictionary that maps the word indices to strings(used for text decoding)\n",
    "def sample_next(predictions, temperature=1.0): # Function for implementing variable temperature sampling from a probability distribution ( the first code block in the notebook)\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_preds = np.exp(predictions)\n",
    "    predictions = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "class TextGenerator(keras.callbacks.Callback): # We will use this as a callback to fit our model\n",
    "    def __init__(self,\n",
    "        prompt, # prompt that will be model input for text generation\n",
    "        generate_length, # number of words to generate\n",
    "        model_input_length, # length of the inputs we used to train the model\n",
    "        temperatures=(1.,), # range of temperatures\n",
    "        print_freq=1):\n",
    "        self.prompt = prompt\n",
    "        self.generate_length = generate_length\n",
    "        self.model_input_length = model_input_length\n",
    "        self.temperatures = temperatures\n",
    "        self.print_freq = print_freq\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.print_freq != 0:\n",
    "            return\n",
    "        for temperature in self.temperatures:\n",
    "            print(\"== Generating with temperature\", temperature)\n",
    "            sentence = self.prompt # our prompt(input for text generation) will be the initial part of the final output sequence \n",
    "        for i in range(self.generate_length):\n",
    "            tokenized_sentence = text_vectorization([sentence])\n",
    "            predictions = self.decoder_model(tokenized_sentence) # next word prediction probability distribution by the model\n",
    "            next_token = sample_next(predictions[0, i, :]) # using the prediction of the model with the temperature range to get the next token\n",
    "            sampled_token = tokens_index[next_token]\n",
    "            sentence += \" \" + sampled_token # Adding the next word in the sentence ( which acts as the input prompt for the next epoch)\n",
    "        print(sentence)\n",
    "\n",
    "prompt = \"This movie\" # Initial prompt\n",
    "text_gen_callback = TextGenerator( #Callback function will call this Text Generator class\n",
    "prompt,\n",
    "generate_length=50,\n",
    "model_input_length=sequence_length,\n",
    "temperatures=(0.2, 0.5, 0.7, 1., 1.5)) # Range of temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocabulary - built manually to avoid the issue\n",
    "\n",
    "def get_vocabulary(self):\n",
    "        # _layer.get_vocabulary\n",
    "        keys, values = self._lookup_layer.lookup_table.export()\n",
    "        # print(self._lookup_layer.lookup_table.export())\n",
    "        vocab = []\n",
    "        for i in keys : \n",
    "            try :\n",
    "                vocab.append(i.numpy().decode('utf-8'))\n",
    "            except :\n",
    "                vocab.append(i.numpy().decode('ISO-8859-1'))\n",
    "        return vocab\n",
    "\n",
    "# Another function\n",
    "\n",
    "def _get_vocabulary():\n",
    "    keys, values = vectorize_layer._index_lookup_layer._table_handler.data()\n",
    "    return [x.decode('utf-8', errors='ignore') for _, x in sorted(zip(values, keys))]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
