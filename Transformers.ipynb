{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention mechanism steps : 1. Calculating the attention scores by dot product of the target word vector with each word vector present in the sentence\n",
    "# 2. Calculating the sum of all the word vectors present in the sentence by their relevance score(attention scores). This will create our new target word vector, which will also contain \n",
    "# information about the surrounding of the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self attention mechanism\n",
    "# Steps performed : Step 1: Initialing the output array\n",
    "# Step 2 : Iterating through the input sequence so each word can once be the target word\n",
    "# Step 3 : \"scores\" is the array that will contain the attention scores for target word with each word vector present in the sequence - Initialized \n",
    "# Step 4 : If input sequence is [[1,2,3],[4,5,6],[7,8,9]] , then\n",
    "#For the first token [1, 2, 3]: -- Target token\n",
    "#Dot product with itself: 1*1 + 2*2 + 3*3 = 1 + 4 + 9 = 14\n",
    "#Dot product with the second token [4, 5, 6]: 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n",
    "#Dot product with the third token [7, 8, 9]: 1*7 + 2*8 + 3*9 = 7 + 16 + 27 = 50\n",
    "#So, the attention scores for the first token [1, 2, 3] are [14, 32, 50].\n",
    "# Step 5 : Once we get the attention scores, we scale them using np.sqrt and normalize them using softmax activation\n",
    "# Step 6 : We get the weighted vectors of the attention scores as the new vector representation\n",
    "# Target token --> [1,2,3]\n",
    "# Attention scores --> [14,32,50] --> sum = 14 + 32 + 50 = 96   \n",
    "# Therefore, weighted vector_1 of 1st token --> [14/96,32/96,50/96] => [1/6, 1/3, 1/2]\n",
    "# Similarly, weighted vector_2 of 1st token --> [8/24, 10/24, 12/24]\n",
    "# Similarly, weighted vector_3 of 1st token --> [35/96, 40/96, 45/96]\n",
    "\n",
    "# Therfore, the new pivot representation will be [1/6 + 8/24 + 35/96, 1/3 + 10/24 + 40/96, 1/2 + 12/24 + 45/96]  -> For 1st token\n",
    "\n",
    "# Similarly, the new pivot representation will be calculated for the next word in the input sequence as the target token\n",
    "# Input sequence is a sequuence with vector representation of the words\n",
    "def self_attention(input_sequence):\n",
    "    output = np.zeros(shape=input_sequence) # Initializing the output\n",
    "    for i , pivot_vector in enumerate(input_sequence): # pivot_vector is each token in the sentence ( Each token in the sentence will be a target token once)\n",
    "        scores = np.zeros(shape=(len(input_sequence),)) # Initializing the scores (Depends on the number of tokens in input_sequence)\n",
    "        for j,vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector,vector.T) # Used for computing the attention scores as a dot product between the pivot vector and the remaining vectors (vector.T means transpose )\n",
    "        scores /= np.sqrt(input_sequence.shape[1]) # Scaling \n",
    "        scores = softmax(scores) # Softmax activation function applied\n",
    "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
    "        for j,vector in enumerate(input_sequence):\n",
    "            new_pivot_representation += scores[j] * vector # This vector representation of each word will contain the information about the surrounding words as well\n",
    "        output[i] = new_pivot_representation\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation behind new pivot representation\n",
    "For the first token [1, 2, 3]:\n",
    "\n",
    "Weight for the first vector [1, 2, 3]:\n",
    "\n",
    "    weight_1 = 14 / (14 + 32 + 50 + 68) = 14 / 164 ≈ 0.0854\n",
    "\n",
    "Weight for the second vector [4, 5, 6]:\n",
    "\n",
    "    weight_2 = 32 / (14 + 32 + 50 + 68) = 32 / 164 ≈ 0.1951\n",
    "\n",
    "Weight for the third vector [7, 8, 9]:\n",
    "\n",
    "    weight_3 = 50 / (14 + 32 + 50 + 68) = 50 / 164 ≈ 0.3049\n",
    "\n",
    "Weight for the fourth vector [10, 11, 12]:\n",
    "\n",
    "    weight_4 = 68 / (14 + 32 + 50 + 68) = 68 / 164 ≈ 0.4146\n",
    "\n",
    "Weighted vector for the first vector [1, 2, 3]:\n",
    "\n",
    "    weighted_vector_1 = [0.0854, 0.1708, 0.2562]\n",
    "\n",
    "Weighted vector for the second vector [4, 5, 6]:\n",
    "\n",
    "    weighted_vector_2 = [0.7804, 0.9756, 1.1707]\n",
    "\n",
    "Weighted vector for the third vector [7, 8, 9]:\n",
    "\n",
    "    weighted_vector_3 = [2.1348, 2.4393, 2.7439]\n",
    "\n",
    "Weighted vector for the fourth vector [10, 11, 12]:\n",
    "\n",
    "    weighted_vector_4 = [4.1557, 4.5713, 4.9868]\n",
    "\n",
    "\n",
    "Sum of weighted vectors:\n",
    "\n",
    "    [0.0854 + 0.7804 + 2.1348 + 4.1557, \n",
    "     0.1708 + 0.9756 + 2.4393 + 4.5713,\n",
    "     0.2562 + 1.1707 + 2.7439 + 4.9868]\n",
    "\n",
    "New pivot representation:\n",
    "\n",
    "    [6.4563, 7.6974, 9.9377]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weight_2 = 32 / (14 + 32 + 50 + 68) = 32 / 164 ≈ 0.1951 \n",
    "\n",
    "Vector_2 = [4 , 5 , 6]\n",
    "\n",
    "Weighted vector for the second vector:\n",
    "[4 * 0.1951, 5 * 0.1951, 6 * 0.1951]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
